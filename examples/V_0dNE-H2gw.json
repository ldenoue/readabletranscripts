{"videoId":"V_0dNE-H2gw","title":"Run ALL Your AI Locally in Minutes (LLMs, RAG, and more)","description":"Update! Follow up video for deploying this app to the cloud! \n\nhttps://youtu.be/259KgP3GbdE?si=nUt90VMv63iVMQMe\n\nArtificial Intelligence is no doubt the future of not just software development but the whole world. And I'm on a mission to master it - focusing first on mastering AI Agents.\n\nIn this video I show you an INCREDIBLE packaged local AI solution with Ollama for the LLMs, Qdrant for RAG, Postgres for the SQL database, and n8n for the no code workflow automations. This package is SO easy to set up and use! I walk you through the setup quick and then even show you how to use it to build a RAG AI Agent that runs entirely locally!\n\nOne really important thing to keep in mind is that while I created a no code AI agent with n8n in this video, this whole local AI setup could be used to actually code your AI agents! It doesn't have to be \"no code\" n8n for the actual agent. A custom coded RAG AI agent with n8n for the accompanying workflow automations (agent tools!) is another fantastic use of this local AI package.\n\nHardware: I am actually using a laptop for this video - with a 3070 laptop GPU and an i7-12700H CPU with 14 cores and 20 threads. So nothing crazy at all! It takes up to 5 minutes for me to get a response from Llama 3.1 8b including RAG. Definitely not fast enough for a chatbot though those kind of speeds could be okay for some agentic workloads. Typically though you will want a GPU with at least 8GB of VRAM for an 8b model, and at least 16GB of VRAM for a 70b model like Llama 3.1 70b.\n\n00:00 - 01:17 - Unveiling the Local AI Package\n01:18 - 06:38 - Installing the Local AI Package\n06:39 - 07:45 - Exploring the Docker Containers\n07:46 - 18:07 - Creating a Local RAG AI Agent\n18:08 - 19:18 - Testing the Local RAG AI Agent\n19:19 - 20:19 - Outro + Future Ideas\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nMy version of the local AI starter kit with my improvements, as well as the template for the n8n RAG AI Agent workflow I created in the video, can be found here:\n\nhttps://github.com/coleam00/ai-agents-masterclass/tree/main/local-ai-packaged\n\nLink to the Local AI Starter Kit by n8n:\n\nhttps://github.com/n8n-io/self-hosted-ai-starter-kit\n\nI recommended GitHub Desktop and Docker Desktop in the video - here are the links for both:\n\nhttps://desktop.github.com/download/\nhttps://www.docker.com/products/docker-desktop/\n\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\nJoin me as I push the limits of what is possible with AI. I'll be uploading videos at least two times a week - Sundays and Wednesdays at 7:00 PM CDT! Sundays and Wednesdays are for everything AI, focusing on providing insane and practical educational value. I will also post sometimes on Fridays at 7:00 PM CDT - specifically for platform showcases - sometimes sponsored, always creative in approach!","chapters":[{"text":"Unveiling the Local AI Package","start":0},{"text":"Installing the Local AI Package","start":78000},{"text":"Exploring the Docker Containers","start":399000},{"text":"Creating a Local RAG AI Agent","start":466000},{"text":"Testing the Local RAG AI Agent","start":1088000},{"text":"Outro + Future Ideas","start":1159000}],"viewCount":"258522","duration":1219,"publishDate":"2024-09-15T17:00:12-07:00","thumbnail":"https://img.youtube.com/vi/V_0dNE-H2gw/mqdefault.jpg","translationLanguages":{"ab":"Abkhazian","aa":"Afar","af":"Afrikaans","ak":"Akan","sq":"Albanian","am":"Amharic","ar":"Arabic","hy":"Armenian","as":"Assamese","ay":"Aymara","az":"Azerbaijani","bn":"Bangla","ba":"Bashkir","eu":"Basque","be":"Belarusian","bho":"Bhojpuri","bs":"Bosnian","br":"Breton","bg":"Bulgarian","my":"Burmese","ca":"Catalan","ceb":"Cebuano","zh-Hans":"Chinese (Simplified)","zh-Hant":"Chinese (Traditional)","co":"Corsican","hr":"Croatian","cs":"Czech","da":"Danish","dv":"Divehi","nl":"Dutch","dz":"Dzongkha","en":"English","eo":"Esperanto","et":"Estonian","ee":"Ewe","fo":"Faroese","fj":"Fijian","fil":"Filipino","fi":"Finnish","fr":"French","gaa":"Ga","gl":"Galician","lg":"Ganda","ka":"Georgian","de":"German","el":"Greek","gn":"Guarani","gu":"Gujarati","ht":"Haitian Creole","ha":"Hausa","haw":"Hawaiian","iw":"Hebrew","hi":"Hindi","hmn":"Hmong","hu":"Hungarian","is":"Icelandic","ig":"Igbo","id":"Indonesian","iu":"Inuktitut","ga":"Irish","it":"Italian","ja":"Japanese","jv":"Javanese","kl":"Kalaallisut","kn":"Kannada","kk":"Kazakh","kha":"Khasi","km":"Khmer","rw":"Kinyarwanda","ko":"Korean","kri":"Krio","ku":"Kurdish","ky":"Kyrgyz","lo":"Lao","la":"Latin","lv":"Latvian","ln":"Lingala","lt":"Lithuanian","lua":"Luba-Lulua","luo":"Luo","lb":"Luxembourgish","mk":"Macedonian","mg":"Malagasy","ms":"Malay","ml":"Malayalam","mt":"Maltese","gv":"Manx","mi":"MƒÅori","mr":"Marathi","mn":"Mongolian","mfe":"Morisyen","ne":"Nepali","new":"Newari","nso":"Northern Sotho","no":"Norwegian","ny":"Nyanja","oc":"Occitan","or":"Odia","om":"Oromo","os":"Ossetic","pam":"Pampanga","ps":"Pashto","fa":"Persian","pl":"Polish","pt":"Portuguese","pt-PT":"Portuguese (Portugal)","pa":"Punjabi","qu":"Quechua","ro":"Romanian","rn":"Rundi","ru":"Russian","sm":"Samoan","sg":"Sango","sa":"Sanskrit","gd":"Scottish Gaelic","sr":"Serbian","crs":"Seselwa Creole French","sn":"Shona","sd":"Sindhi","si":"Sinhala","sk":"Slovak","sl":"Slovenian","so":"Somali","st":"Southern Sotho","es":"Spanish","su":"Sundanese","sw":"Swahili","ss":"Swati","sv":"Swedish","tg":"Tajik","ta":"Tamil","tt":"Tatar","te":"Telugu","th":"Thai","bo":"Tibetan","ti":"Tigrinya","to":"Tongan","ts":"Tsonga","tn":"Tswana","tum":"Tumbuka","tr":"Turkish","tk":"Turkmen","uk":"Ukrainian","ur":"Urdu","ug":"Uyghur","uz":"Uzbek","ve":"Venda","vi":"Vietnamese","war":"Waray","cy":"Welsh","fy":"Western Frisian","wo":"Wolof","xh":"Xhosa","yi":"Yiddish","yo":"Yoruba","zu":"Zulu"},"defaultLanguage":"en","en":{"chunks":[{"text":"have you ever wished for a single","start":80,"end":3600,"dur":3520},{"text":"package that you could easily install","start":1640,"end":5279,"dur":3639},{"text":"that has everything you need for local","start":3600,"end":8120,"dur":4520},{"text":"AI well I have good news for you today","start":5279,"end":9920,"dur":4641},{"text":"because I have exactly what you are","start":8119.999999999999,"end":12080,"dur":3960},{"text":"looking for I have actually never been","start":9920,"end":14120,"dur":4200},{"text":"so excited to make a video on something","start":12080,"end":15920,"dur":3840},{"text":"before today I'm going to show you an","start":14120,"end":18080,"dur":3960},{"text":"incredible package for local AI","start":15920,"end":20600,"dur":4680},{"text":"developed by the n8n team and this thing","start":18080,"end":23199,"dur":5119},{"text":"has it all it's got old llama for the","start":20600,"end":26240,"dur":5640},{"text":"llms quadrant for the vector database","start":23199,"end":28840,"dur":5641},{"text":"postgress for the SQL database and then","start":26240,"end":31399,"dur":5159},{"text":"n8n to tie it Al together with workflow","start":28840,"end":33960,"dur":5120},{"text":"automations this thing is absolutely","start":31399,"end":35360,"dur":3961},{"text":"incredible and I'm going to show you how","start":33960,"end":37600,"dur":3640},{"text":"to set it up in just minutes then I'll","start":35360,"end":39360,"dur":4000},{"text":"even show you how to extend it to make","start":37600,"end":42160,"dur":4560},{"text":"it better and use it to create a full","start":39360,"end":45120,"dur":5760},{"text":"rag AI agent in n8n so stick around","start":42160,"end":47160,"dur":5000},{"text":"because I have a lot of value for you","start":45120,"end":49879,"dur":4759},{"text":"today running your own AI infrastructure","start":47160,"end":51920,"dur":4760},{"text":"is the way of the future especially","start":49879,"end":54160,"dur":4281},{"text":"because of how accessible is becoming","start":51920,"end":56680,"dur":4760},{"text":"and because open-source models like","start":54160,"end":58480,"dur":4320},{"text":"llama are getting to the point where","start":56680,"end":60160,"dur":3480},{"text":"they're so powerful that they're","start":58480,"end":62199,"dur":3719},{"text":"actually able to compete with close","start":60160,"end":65439,"dur":5279},{"text":"Source models like GPT and clad so now","start":62199,"end":67200,"dur":5001},{"text":"is the time to jump on this and what I'm","start":65438.99999999999,"end":69439,"dur":4000},{"text":"about to show you is an excellent start","start":67200,"end":71560,"dur":4360},{"text":"to doing so and at the end of this video","start":69439,"end":73119,"dur":3680},{"text":"I'll even talk about how I'm going to","start":71560,"end":75240,"dur":3680},{"text":"extend this package in the near future","start":73119,"end":78000,"dur":4881},{"text":"just for you to make it even better all","start":75240,"end":79479,"dur":4239},{"text":"right so here we are in the GitHub","start":78000,"end":81479,"dur":3479},{"text":"repository for the self-hosted AI","start":79479,"end":84840,"dur":5361},{"text":"starter kit by n8n now this repo is","start":81479,"end":87079,"dur":5600},{"text":"really basic and I love it there","start":84840,"end":88680,"dur":3840},{"text":"are basically just two files that we","start":87079,"end":90479,"dur":3400},{"text":"have to care about here we have our","start":88680,"end":92439,"dur":3759},{"text":"environment variable file where we'll","start":90479,"end":93759,"dur":3280},{"text":"set credentials for things like","start":92439,"end":95640,"dur":3201},{"text":"postgress and then we have Docker","start":93759,"end":98119,"dur":4360},{"text":"compose the caml file here where we'll","start":95640,"end":99600,"dur":3960},{"text":"basically be bringing in everything","start":98119,"end":101360,"dur":3241},{"text":"together like postgress quadrant and","start":99600,"end":103920,"dur":4320},{"text":"olama to have a single package for our","start":101360,"end":106200,"dur":4840},{"text":"local AI now the first thing that I want","start":103920,"end":108640,"dur":4720},{"text":"to mention here is that this read me has","start":106200,"end":110159,"dur":3959},{"text":"instructions for how to install","start":108640,"end":112399,"dur":3759},{"text":"everything yourself but honestly it's","start":110159,"end":113880,"dur":3721},{"text":"quite lacking and there's a couple of","start":112399,"end":115759,"dur":3360},{"text":"holes that I want to fill in here with","start":113880,"end":117479,"dur":3599},{"text":"ways to extend it to really make it what","start":115759,"end":119439,"dur":3680},{"text":"I think that you need and so I'll go","start":117479,"end":120600,"dur":3121},{"text":"through that a little bit and we'll","start":119439,"end":122280,"dur":2841},{"text":"actually get this installed on our","start":120600,"end":123880,"dur":3280},{"text":"computer now there are a couple of","start":122280,"end":126200,"dur":3920},{"text":"dependencies before you start basically","start":123880,"end":127920,"dur":4040},{"text":"you just need git and Docker so I'd","start":126200,"end":130039,"dur":3839},{"text":"recommend installing GitHub desktop and","start":127920,"end":131760,"dur":3840},{"text":"then Docker desktop as well because this","start":130038.99999999999,"end":133959,"dur":3920},{"text":"also has Docker compose with it which is","start":131760,"end":135599,"dur":3839},{"text":"what we need to bring everything","start":133959,"end":138120,"dur":4161},{"text":"together for one package so with that we","start":135599,"end":139599,"dur":4000},{"text":"can go ahead and get started downloading","start":138120,"end":141640,"dur":3520},{"text":"this on our computer so the first thing","start":139599,"end":143560,"dur":3961},{"text":"you want to do to download this code is","start":141640,"end":145720,"dur":4080},{"text":"copy the get clone command here with the","start":143560,"end":148319,"dur":4759},{"text":"URL of the repository you'll go into a","start":145720,"end":150200,"dur":4480},{"text":"terminal then and then paste in this","start":148319,"end":152360,"dur":4041.0000000000005},{"text":"command for me I've already cloned this","start":150200,"end":153720,"dur":3520},{"text":"that's why I get this error message but","start":152360,"end":155680,"dur":3320},{"text":"you're going to get this code downloaded","start":153720,"end":157120,"dur":3400},{"text":"on your computer and then you can change","start":155680,"end":160120,"dur":4440},{"text":"your directory into this new repository","start":157120,"end":162239,"dur":5119},{"text":"that you've pulled and so with this we","start":160120,"end":164159,"dur":4038.9999999999995},{"text":"can now go and edit the files in any","start":162239,"end":166480,"dur":4241},{"text":"editor of our choice I like using VSS","start":164159,"end":168959,"dur":4800},{"text":"code and so if you have VSS code as well","start":166480,"end":170879,"dur":4399},{"text":"you can just type in code Dot and this","start":168959,"end":173040,"dur":4081.0000000000005},{"text":"is going to pull up everything in Visual","start":170879,"end":174840,"dur":3961},{"text":"Studio code now the official","start":173040,"end":176519,"dur":3479},{"text":"instructions in the readme that we just","start":174840,"end":178480,"dur":3640},{"text":"saw would tell you at this point to run","start":176519,"end":180040,"dur":3521},{"text":"everything with the docker compos post","start":178480,"end":182200,"dur":3720},{"text":"command now that is not actually the","start":180040,"end":184000,"dur":3960},{"text":"right Next Step I'm not really sure why","start":182200,"end":185959,"dur":3759},{"text":"they say that cuz we have to actually go","start":184000,"end":187799,"dur":3799},{"text":"and edit a couple of things in the code","start":185959,"end":190040,"dur":4081.0000000000005},{"text":"to make it customized for us and that","start":187799,"end":192599,"dur":4800},{"text":"starts with the EnV file so you're going","start":190040,"end":194400,"dur":4360},{"text":"to want to go into your EnV file I've","start":192599,"end":197159,"dur":4560},{"text":"just made a env. example file in this","start":194400,"end":198560,"dur":4160},{"text":"case because I already have my","start":197159,"end":200280,"dur":3121},{"text":"credentials set up so you'll go into","start":198560,"end":202760,"dur":4200},{"text":"your EnV and then set up your postgress","start":200280,"end":205000,"dur":4720},{"text":"username and password the database name","start":202760,"end":207120,"dur":4360},{"text":"and then also a couple of n8n Secrets","start":205000,"end":208760,"dur":3760},{"text":"these can be whatever you want just make","start":207120,"end":210280,"dur":3160},{"text":"sure that they are very here and","start":208760,"end":212319,"dur":3559},{"text":"basically just a long alpha numeric","start":210280,"end":214439,"dur":4159},{"text":"string and then with that we can go into","start":212319,"end":216599,"dur":4280},{"text":"our Docker compose file and here's where","start":214439,"end":218760,"dur":4321},{"text":"I want to make a couple of extensions to","start":216599,"end":220400,"dur":3801},{"text":"really fill in the gaps so the couple of","start":218760,"end":222200,"dur":3440},{"text":"things that were missing in the original","start":220400,"end":225080,"dur":4680},{"text":"Docker compose file first of all for","start":222200,"end":227680,"dur":5480},{"text":"some reason the postgress container","start":225080,"end":229560,"dur":4480},{"text":"doesn't have the port exposed by default","start":227680,"end":231360,"dur":3680},{"text":"so you can't actually go and use","start":229560,"end":233840,"dur":4280},{"text":"postgress as your database in an NN","start":231360,"end":236560,"dur":5200},{"text":"workflow I think n uses postgress","start":233840,"end":238200,"dur":4360},{"text":"internally which is why it's set up like","start":236560,"end":239480,"dur":2920},{"text":"that initially but we want to actually","start":238200,"end":242120,"dur":3920},{"text":"be a to use postgress for our chat","start":239480,"end":243599,"dur":4119},{"text":"memory for our agents and so I'm going","start":242120,"end":245959,"dur":3839},{"text":"to show you how to do that basically all","start":243599,"end":248159,"dur":4560},{"text":"you have to do is go down to the","start":245959,"end":250400,"dur":4441},{"text":"postgress service here and then just add","start":248159,"end":252760,"dur":4601},{"text":"these two lines of code right here ports","start":250400,"end":254200,"dur":3800},{"text":"and then just a single item where we","start":252760,"end":258479,"dur":5719},{"text":"have 5432 map to the port 5432 inside","start":254200,"end":259840,"dur":5640},{"text":"the container and that way we can go","start":258478.99999999997,"end":263160,"dur":4681},{"text":"Local Host 5432 and access postgress so","start":259839.99999999997,"end":265040,"dur":5200},{"text":"that is super important otherwise","start":263160,"end":266320,"dur":3160},{"text":"we won't actually be able to access it","start":265040,"end":268000,"dur":2960},{"text":"within an NA end workflow we're going to","start":266320,"end":269720,"dur":3400},{"text":"be doing that later when we build the","start":268000,"end":272360,"dur":4360},{"text":"rag AI agent now the other thing that we","start":269720,"end":274759,"dur":5039},{"text":"want to do is we want to use olama for","start":272360,"end":276680,"dur":4320},{"text":"our embeddings for our Vector database","start":274759,"end":279160,"dur":4401},{"text":"as well now the base command when we","start":276680,"end":281919,"dur":5239},{"text":"initialize olama is just this part right","start":279160,"end":285000,"dur":5840},{"text":"here so we sleep for 3 seconds and then","start":281919,"end":287759,"dur":5840},{"text":"we pull llama 3.1 with oama so that's","start":285000,"end":290120,"dur":5120},{"text":"why we have llama 3.1 Available To Us by","start":287759,"end":292280,"dur":4521},{"text":"default but what I've added here is","start":290120,"end":295639,"dur":5519},{"text":"another line to pull one of the olama","start":292280,"end":297639,"dur":5359},{"text":"embedding models and we need this if we","start":295639,"end":300160,"dur":4521},{"text":"want to be able to use AMA for our","start":297639,"end":302800,"dur":5161},{"text":"Rag and so I've added this line as well","start":300160,"end":304680,"dur":4520},{"text":"that is very key so that is","start":302800,"end":306199,"dur":3399},{"text":"literally everything that you have to","start":304680,"end":307880,"dur":3200},{"text":"change in the code to get this to work","start":306199,"end":309120,"dur":2921},{"text":"and I'll even have a link in the","start":307880,"end":311360,"dur":3480},{"text":"description of this video to my version","start":309120,"end":312919,"dur":3799},{"text":"of this you can pull that directly if","start":311360,"end":314639,"dur":3279},{"text":"you want to have all the customizations","start":312919,"end":316520,"dur":3601},{"text":"that we just went over here and with","start":314639,"end":318160,"dur":3521},{"text":"that we can go ahead and actually start","start":316520,"end":320479,"dur":3959},{"text":"it with Docker compose and so the","start":318160,"end":322280,"dur":4120},{"text":"installation instructions in the readme","start":320479,"end":323680,"dur":3201},{"text":"are actually kind of useful here because","start":322280,"end":325160,"dur":2880},{"text":"there's a slightly different Docker","start":323680,"end":326919,"dur":3239},{"text":"compose command that you want to run","start":325160,"end":328800,"dur":3640},{"text":"based on your architecture so if you","start":326919,"end":330319,"dur":3400},{"text":"have a Nvidia G","start":328800,"end":331840,"dur":3040},{"text":"you can follow these instructions which","start":330319,"end":333479,"dur":3160},{"text":"are a bit more complicated but if you","start":331840,"end":335400,"dur":3560},{"text":"want to you can and then you can run","start":333479,"end":338199,"dur":4720},{"text":"with a GPU Nvidia profile and then if","start":335400,"end":340240,"dur":4840},{"text":"you are a Mac User you follow this","start":338199,"end":341960,"dur":3761},{"text":"Command right here and then for everyone","start":340240,"end":343319,"dur":3079},{"text":"else like what I'm going to use in this","start":341960,"end":345319,"dur":3359},{"text":"case even though I have a Nvidia GPU","start":343319,"end":347039,"dur":3720},{"text":"I'll just keep it simple with Docker","start":345319,"end":350560,"dur":5241},{"text":"compose d-profile CPU up and so we'll","start":347039,"end":352759,"dur":5720},{"text":"copy this command go into our terminal","start":350560,"end":355400,"dur":4840},{"text":"here and paste it in and in my case I","start":352759,"end":356960,"dur":4201},{"text":"have already created all these","start":355400,"end":358280,"dur":2880},{"text":"containers and so it's going to run","start":356960,"end":360240,"dur":3280},{"text":"really fast for me but in your","start":358280,"end":361919,"dur":3639},{"text":"case it's going to have to pull each of","start":360240,"end":365240,"dur":5000},{"text":"the images for olama postgress n8n and","start":361919,"end":367240,"dur":5321},{"text":"quadrant and then start them all up and","start":365240,"end":368520,"dur":3280},{"text":"it'll take a little bit because I also","start":367240,"end":370759,"dur":3519},{"text":"have to do things like pulling llama 3.1","start":368520,"end":373400,"dur":4880},{"text":"for the old llama container and so in my","start":370759,"end":374759,"dur":4000},{"text":"case it's going to blast through this","start":373400,"end":375919,"dur":2519},{"text":"pretty quick because it's already done a","start":374759,"end":377800,"dur":3041},{"text":"lot of this I did that on purpose so it","start":375919,"end":379479,"dur":3560},{"text":"can be a quicker walkthrough for you","start":377800,"end":381160,"dur":3360},{"text":"here but you can see all the","start":379479,"end":382400,"dur":2921},{"text":"different containers the different","start":381160,"end":384599,"dur":3439},{"text":"colors here that are running everything","start":382400,"end":386319,"dur":3919},{"text":"to set me up for each of the different","start":384599,"end":387919,"dur":3320},{"text":"services and so like right here for","start":386319,"end":390880,"dur":4561},{"text":"example it pulled llama 3.1 and then","start":387919,"end":392639,"dur":4720},{"text":"right here it pulled the embedding model","start":390880,"end":395120,"dur":4240},{"text":"that I chose from AMA as well and so","start":392639,"end":396720,"dur":4081.0000000000005},{"text":"at this point it's basically done so I'm","start":395120,"end":398120,"dur":3000},{"text":"going to pause here and come back when","start":396720,"end":399960,"dur":3240},{"text":"everything is ready all right so","start":398120,"end":401280,"dur":3160},{"text":"everything is good to go and now I'm","start":399960,"end":402759,"dur":2799},{"text":"going to actually take you in a Docker","start":401280,"end":405360,"dur":4080},{"text":"so we can see all of this running live","start":402759,"end":406759,"dur":4000},{"text":"so you're going to want to open up your","start":405360,"end":408720,"dur":3360},{"text":"Docker desktop and then you'll see one","start":406759,"end":411240,"dur":4481},{"text":"record here for the self-hosted AI","start":408720,"end":412800,"dur":4080},{"text":"starter kit you can click on this button","start":411240,"end":414800,"dur":3560},{"text":"on the left hand side to expand it and","start":412800,"end":416479,"dur":3679},{"text":"then we can see every container that is","start":414800,"end":419039,"dur":4239},{"text":"currently running or ran for the setup","start":416479,"end":420759,"dur":4280},{"text":"so they're going to four containers each","start":419039,"end":422280,"dur":3241},{"text":"running for one of our different local","start":420759,"end":424599,"dur":3840},{"text":"AI services and we can actually click","start":422280,"end":426240,"dur":3960},{"text":"into each one of them which is super","start":424599,"end":428319,"dur":3720},{"text":"cool because we can see the output of","start":426240,"end":430720,"dur":4480},{"text":"each container and even go to the exec","start":428319,"end":433319,"dur":5000},{"text":"tab to run Linux commands within each of","start":430720,"end":434720,"dur":4000},{"text":"these containers and so you can actually","start":433319,"end":436960,"dur":3641},{"text":"do things in real time as well without","start":434720,"end":438720,"dur":4000},{"text":"having to restart the containers you can","start":436960,"end":441440,"dur":4480},{"text":"go into the postgress container and run","start":438720,"end":443560,"dur":4840},{"text":"commands to query your tables and stuff","start":441440,"end":444960,"dur":3520},{"text":"you can go into actually I'll show you","start":443560,"end":446160,"dur":2600},{"text":"this really quick you can go into the","start":444960,"end":449560,"dur":4600},{"text":"olama container and you can pull in real","start":446160,"end":451680,"dur":5520},{"text":"time like if I want to go to exec here I","start":449560,"end":455199,"dur":5639},{"text":"can do AMA pull llama","start":451680,"end":458639,"dur":6959},{"text":"3.1 if I can spell it right 70b so I can","start":455199,"end":460560,"dur":5361},{"text":"pull models in real time and have those","start":458639,"end":462840,"dur":4201},{"text":"updated and available to me in n8n","start":460560,"end":464520,"dur":3960},{"text":"without having to actually restart","start":462840,"end":466639,"dur":3799},{"text":"anything which is super cool all","start":464520,"end":468319,"dur":3799},{"text":"right so now is the really fun","start":466639,"end":470400,"dur":3761},{"text":"part because we get to use all the local","start":468319,"end":472240,"dur":3921},{"text":"infrastructure that we spun up just now","start":470400,"end":475199,"dur":4799},{"text":"to create a fully local rag AI agent","start":472240,"end":477720,"dur":5480},{"text":"within n8n and so to access your new","start":475199,"end":480599,"dur":5400},{"text":"self-hosted n8n you can just go to Local","start":477720,"end":483680,"dur":5960},{"text":"Host Port 5678 and the way that you know","start":480599,"end":485639,"dur":5040},{"text":"that this is the URL is either through","start":483680,"end":488000,"dur":4320},{"text":"the docker logs for your n container or","start":485639,"end":490080,"dur":4441},{"text":"in the readme that we went over that","start":488000,"end":492440,"dur":4440},{"text":"was in the GitHub repository we cloned","start":490080,"end":494080,"dur":4000},{"text":"and with that we can dive into this","start":492440,"end":496520,"dur":4080},{"text":"workflow that I created to use postgress","start":494080,"end":499120,"dur":5040},{"text":"for the chat memory quadrant for Rag and","start":496520,"end":501720,"dur":5200},{"text":"olama for the llm and the embedding","start":499120,"end":504560,"dur":5440},{"text":"model and so this is a full rag AI agent","start":501720,"end":505879,"dur":4159},{"text":"that I've already built out I don't want","start":504560,"end":507360,"dur":2800},{"text":"to build it from scratch just because I","start":505879,"end":509520,"dur":3641},{"text":"want this to be a quicker smooth walk","start":507360,"end":511159,"dur":3799},{"text":"through for you but I'll still go step","start":509520,"end":512800,"dur":3280},{"text":"by step through everything that I set up","start":511159,"end":514519,"dur":3360},{"text":"here and so that you can understand it","start":512799.99999999994,"end":516439.99999999994,"dur":3640},{"text":"for yourself and also just steal this","start":514519,"end":518320,"dur":3801},{"text":"from me CU I'm going to have this in the","start":516440.00000000006,"end":520279.00000000006,"dur":3839},{"text":"description link as well so you can pull","start":518320.00000000006,"end":521760.00000000006,"dur":3440},{"text":"this workflow and bring it into your own","start":520279,"end":524240,"dur":3961},{"text":"n8n instance and so with that we can go","start":521760,"end":526560,"dur":4800},{"text":"ahead and get started so there are two","start":524240,"end":528600,"dur":4360},{"text":"parts to this workflow first of all we","start":526560,"end":530839,"dur":4279},{"text":"have the agent itself with the chat","start":528600,"end":532720,"dur":4120},{"text":"interaction here so this chat widget is","start":530839,"end":534640,"dur":3801},{"text":"how we can interact with our agent and","start":532720,"end":536240,"dur":3520},{"text":"then we also have the workflow that is","start":534640,"end":538720,"dur":4080},{"text":"going to bring files from Google Drive","start":536240,"end":541000,"dur":4760},{"text":"into our knowledge base with quadrant","start":538720,"end":543079,"dur":4359},{"text":"and so I'll show the agent first and","start":541000,"end":544959,"dur":3959},{"text":"then I'll dive very quickly into how I","start":543079,"end":547240,"dur":4161},{"text":"have this pipeline set up to pull files","start":544959,"end":549160,"dur":4201},{"text":"in from a Google drive folder into my","start":547240,"end":551240,"dur":4000},{"text":"knowledge base so we have the trigger","start":549160,"end":552959,"dur":3799},{"text":"that I just mentioned there where we","start":551240,"end":555279,"dur":4038.9999999999995},{"text":"have our chat input and that is fed","start":552959,"end":557120,"dur":4161},{"text":"directly into this AI agent where we","start":555279,"end":559240,"dur":3961},{"text":"hook up all the different local stuff","start":557120,"end":561360,"dur":4240},{"text":"and so first of all we have our olama","start":559240,"end":563200,"dur":3960},{"text":"chat model and so I'm referencing llama","start":561360,"end":566240,"dur":4880},{"text":"3.1 colon latest which is the 8 billion","start":563200,"end":568440,"dur":5240},{"text":"parameter model but if you want to do an","start":566240,"end":570160,"dur":3920},{"text":"AMA PLL Within the container like I","start":568440,"end":571680,"dur":3240},{"text":"showed you how to do you can use","start":570160,"end":574120,"dur":3960},{"text":"literally any olama llm right here it is","start":571680,"end":576959,"dur":5279},{"text":"just so simple to set up and then for","start":574120,"end":579320,"dur":5200},{"text":"the credentials here it is very easy you","start":576959,"end":581760,"dur":4801},{"text":"just have to put in this base URL right","start":579320,"end":584640,"dur":5320},{"text":"here it is so important that for the URL","start":581760,"end":585560,"dur":3800},{"text":"you use","start":584640,"end":588160,"dur":3520},{"text":"HTTP and instead of Local Host you","start":585560,"end":591640,"dur":6080},{"text":"reference host. doer. internal otherwise","start":588160,"end":593880,"dur":5720},{"text":"it will not work and then the port for","start":591640,"end":596600,"dur":4960},{"text":"Alama is if you don't change it","start":593880,"end":599040,"dur":5160},{"text":"11434 and you can get this port either","start":596600,"end":601120,"dur":4520},{"text":"in the Docker compost file or in the","start":599040,"end":603279,"dur":4239},{"text":"logs for the AMA container you'll see","start":601120,"end":605600,"dur":4480},{"text":"this in a lot of places and so with that","start":603279,"end":608040,"dur":4761},{"text":"we've got our llm set up for this agent","start":605600,"end":609480,"dur":3880},{"text":"and then for the memory of course we're","start":608040,"end":611120,"dur":3080},{"text":"going to use postgress and so I'll click","start":609480,"end":613079,"dur":3599},{"text":"into this and we're just going to have","start":611120,"end":614800,"dur":3680},{"text":"any kind of table name you have here and","start":613079,"end":616480,"dur":3401},{"text":"N will create this automatically in your","start":614800,"end":619200,"dur":4400},{"text":"postgress database and it'll get the","start":616480,"end":621760,"dur":5280},{"text":"session ID from the previous node and","start":619200,"end":623959,"dur":4759},{"text":"then for the credentials here this is","start":621760,"end":625399,"dur":3639},{"text":"going to be based on what you set in","start":623959,"end":628160,"dur":4201},{"text":"yourb file so we have our host which is","start":625399,"end":630880,"dur":5481},{"text":"host. doer. internal again just like","start":628160,"end":633959,"dur":5799},{"text":"with AMA and then the database name user","start":630880,"end":635519,"dur":4639},{"text":"and password all three of those you","start":633959,"end":637519,"dur":3560},{"text":"defined in your EnV file that we went","start":635519,"end":640000,"dur":4481},{"text":"over earlier and the port for postgress","start":637519,"end":641200,"dur":3681},{"text":"is","start":640000,"end":643480,"dur":3480},{"text":"5432 and so with that we've got our","start":641200,"end":645279,"dur":4078.9999999999995},{"text":"local chat memory set up it is that","start":643480,"end":646920,"dur":3440},{"text":"simple and so we can move on to the last","start":645279,"end":649760,"dur":4481},{"text":"part of this agent which is the tool for","start":646920,"end":652160,"dur":5240},{"text":"rag so we have the vector store tool","start":649760,"end":654760,"dur":5000},{"text":"that we attach to our agent and then we","start":652160,"end":656800,"dur":4640},{"text":"hook in our quadrant Vector store for","start":654760,"end":658200,"dur":3440},{"text":"this and so we're just going to retrieve","start":656800,"end":660399,"dur":3599},{"text":"any documents based on the query that","start":658200,"end":662399,"dur":4199},{"text":"comes into our agent and then for the","start":660399,"end":664480,"dur":4081.0000000000005},{"text":"credentials for Quadrant we just have an","start":662399,"end":666920,"dur":4521},{"text":"API key which this was filled in for me","start":664480,"end":668760,"dur":4280},{"text":"by default so I hope it is for you as","start":666920,"end":670920,"dur":4000},{"text":"well I think it's just the password for","start":668760,"end":672959,"dur":4199},{"text":"the NN instance and then for the","start":670920,"end":675279,"dur":4359},{"text":"quadrant URL this should look very","start":672959,"end":679000,"dur":6041},{"text":"familiar HTTP host. doer. internal and","start":675279,"end":681720,"dur":6441},{"text":"then the port for Quadrant is 6333 again","start":679000,"end":683600,"dur":4600},{"text":"you can get this from the docker compose","start":681720,"end":685720,"dur":4000},{"text":"file because we have to expose that Port","start":683600,"end":687800,"dur":4200},{"text":"make it available or you can get it from","start":685720,"end":690320,"dur":4600},{"text":"the quadrant logs as well","start":687800,"end":691720,"dur":3920},{"text":"and so one other thing that I want to","start":690320,"end":694360,"dur":4040},{"text":"show that is so cool with hosting","start":691720,"end":696959,"dur":5239},{"text":"quadrant locally is if you go to local","start":694360,"end":698360,"dur":4000},{"text":"hostport","start":696959,"end":700519,"dur":3560},{"text":"6333 like I have right here you can see","start":698360,"end":703760,"dur":5400},{"text":"in the top left slash dashboard it's","start":700519,"end":705800,"dur":5281},{"text":"going to take you to your very own","start":703760,"end":708360,"dur":4600},{"text":"self-hosted quadrant dashboard where you","start":705800,"end":710000,"dur":4200},{"text":"can see all your collections your","start":708360,"end":712279,"dur":3919},{"text":"knowledge base basically and you can see","start":710000,"end":713639,"dur":3639},{"text":"all the different vectors that you have","start":712279,"end":716800,"dur":4521},{"text":"in there you can click into visualize","start":713639,"end":718560,"dur":4921},{"text":"and I can actually go and see all my","start":716800,"end":720399,"dur":3599},{"text":"different vectors which this is a","start":718560,"end":722000,"dur":3440},{"text":"document that I already have inserted as","start":720399,"end":723920,"dur":3521},{"text":"I was testing things so you can see","start":722000,"end":725760,"dur":3760},{"text":"all the metadata the contents of each","start":723920,"end":728440,"dur":4520},{"text":"chunk it is so cool so we'll go back","start":725760,"end":730160,"dur":4400},{"text":"to this in a little bit here but just","start":728440,"end":731480,"dur":3040},{"text":"know that like you have so much","start":730160,"end":732880,"dur":2720},{"text":"visibility into your own quadrant","start":731480,"end":734639,"dur":3159},{"text":"instance and you can even go and like","start":732880,"end":736519,"dur":3639},{"text":"run your own queries to get","start":734639,"end":738600,"dur":3961},{"text":"collections or delete vectors or do a","start":736519,"end":741120,"dur":4601},{"text":"search it's just really awesome so","start":738600,"end":743199,"dur":4599},{"text":"yeah hosting quadrant is a beautiful","start":741120,"end":745440,"dur":4320},{"text":"thing and so with that we have our","start":743199,"end":747440,"dur":4241},{"text":"quadrant Vector store and then we're","start":745440,"end":749720,"dur":4280},{"text":"using olama for embeddings using that","start":747440,"end":751760,"dur":4320},{"text":"embedding model that I pulled that I","start":749720,"end":754160,"dur":4440},{"text":"added to the docker compost file and","start":751760,"end":756320,"dur":4560},{"text":"then we're just going to use llama 3.1","start":754160,"end":758639,"dur":4479},{"text":"again to parse the responses that we get","start":756320,"end":760959,"dur":4639},{"text":"from rag when we do our lookups so that","start":758639,"end":763480,"dur":4841},{"text":"is everything for our agent and so we'll","start":760959,"end":765560,"dur":4601},{"text":"test this in a little bit but first I","start":763480,"end":767480,"dur":4000},{"text":"want to actually show you the workflow","start":765560,"end":769800,"dur":4240},{"text":"for ingesting files into our knowledge","start":767480,"end":771680,"dur":4200},{"text":"base and so the way that works is we","start":769800,"end":773639,"dur":3839},{"text":"have two triggers here basically","start":771680,"end":776839,"dur":5159},{"text":"whenever a file is created in a specific","start":773639,"end":779600,"dur":5961},{"text":"folder in Google Drive or if a file is","start":776839,"end":782000,"dur":5161},{"text":"updated in that same folder we want to","start":779600,"end":784120,"dur":4520},{"text":"run this pipeline to download the file","start":782000,"end":785920,"dur":3920},{"text":"and put it into our quadrant Vector","start":784120,"end":788040,"dur":3920},{"text":"database running locally and so that","start":785920,"end":790519,"dur":4599},{"text":"folder that I have right here is this","start":788040,"end":792399,"dur":4359},{"text":"meeting notes folder in my Google Drive","start":790519,"end":794600,"dur":4081.0000000000005},{"text":"and specifically the document that I'm","start":792399,"end":797240,"dur":4841},{"text":"going to use for testing purposes here","start":794600,"end":799279,"dur":4679},{"text":"is these fake meeting notes that I made","start":797240,"end":800839,"dur":3599},{"text":"I just generated something really","start":799279,"end":803040,"dur":3761},{"text":"silly here about a company that is","start":800839,"end":806079,"dur":5240},{"text":"selling robotic pets and AI startup ","start":803040,"end":807480,"dur":4440},{"text":"and so we're going to use this document","start":806079,"end":809120,"dur":3041},{"text":"for our rag I'm not going to do a bunch","start":807480,"end":811040,"dur":3560},{"text":"bunch of different documents because","start":809120,"end":812560,"dur":3440},{"text":"I want to keep this really simple right","start":811040,"end":814320,"dur":3280},{"text":"now but you can definitely do that and","start":812560,"end":816279,"dur":3719},{"text":"the quadrant Vector database can handle","start":814320,"end":818120,"dur":3800},{"text":"that but for now I'm just using this","start":816279,"end":820120,"dur":3841},{"text":"single document and so I'll walk through","start":818120,"end":822079,"dur":3959},{"text":"step by step what this flow actually","start":820120,"end":823920,"dur":3800},{"text":"looks like to ingest this into the","start":822079,"end":826279,"dur":4200},{"text":"vector database and so first of all I'm","start":823920,"end":828079,"dur":4159},{"text":"going to fetch a test event which is","start":826279,"end":830120,"dur":3841},{"text":"going to be the creation of this meeting","start":828079,"end":832320,"dur":4241},{"text":"Note file that I just showed you and","start":830120,"end":833800,"dur":3680},{"text":"then we're going to feed that into this","start":832320,"end":835800,"dur":3480},{"text":"node here which is going to extrapolate","start":833800,"end":838000,"dur":4200},{"text":"a couple of key pieces of information","start":835800,"end":841480,"dur":5680},{"text":"including the file ID and the folder ID","start":838000,"end":843519,"dur":5519},{"text":"and so once we have that I'm going to go","start":841480,"end":845360,"dur":3880},{"text":"on to this next step right here and this","start":843519,"end":848079,"dur":4560},{"text":"is a very important step okay let","start":845360,"end":850360,"dur":5000},{"text":"me just stop here for a second there are","start":848079,"end":853800,"dur":5721},{"text":"a lot of rag tutorials with n8n on","start":850360,"end":856279,"dur":5919},{"text":"YouTube that miss this when you have","start":853800,"end":857680,"dur":3880},{"text":"this Step at the end here I'm just going","start":856279,"end":860240,"dur":3961},{"text":"to skip to the end really quick whether","start":857680,"end":862360,"dur":4680},{"text":"this is super base quadrant pine cone it","start":860240,"end":863959,"dur":3719},{"text":"doesn't matter when you have this","start":862360,"end":867639,"dur":5279},{"text":"inserter it is not an upsert it is just","start":863959,"end":869279,"dur":5320},{"text":"an insert and so what that means","start":867639,"end":871680,"dur":4041.0000000000005},{"text":"is if you reinsert a document you're","start":869279,"end":873680,"dur":4401},{"text":"actually going to have duplicate vectors","start":871680,"end":875759,"dur":4078.9999999999995},{"text":"for that document so if I update a","start":873680,"end":877480,"dur":3800},{"text":"document in Google Drive and it","start":875759,"end":880320,"dur":4561},{"text":"reinserts the vectors into my quadrant","start":877480,"end":882360,"dur":4880},{"text":"Vector database I'm going to have the","start":880320,"end":884199,"dur":3879},{"text":"old vectors for the first time I","start":882360,"end":886440,"dur":4080},{"text":"ingested my document and then new","start":884199,"end":888720,"dur":4521},{"text":"vectors for when I updated the file it","start":886440,"end":891320,"dur":4880},{"text":"does not get rid of the old files or","start":888720,"end":893759,"dur":5039},{"text":"update the vectors in place that is so","start":891320,"end":896199,"dur":4879},{"text":"important to keep in mind and so I'm","start":893759,"end":898639,"dur":4880},{"text":"giving a lot of value to you right here","start":896199,"end":900800,"dur":4601},{"text":"by including this node and it's actually","start":898639,"end":902519,"dur":3880},{"text":"custom code because there's not a way to","start":900800,"end":905399,"dur":4599},{"text":"do it without code in n8n but it is all","start":902519,"end":907000,"dur":4481},{"text":"good because you can just copy this from","start":905399,"end":908880,"dur":3481},{"text":"me I'm going to have a link to this","start":907000,"end":910440,"dur":3440},{"text":"workflow in the description like I said","start":908880,"end":912160,"dur":3280},{"text":"so you can just download this and bring","start":910440,"end":914600,"dur":4160},{"text":"it into your own n8n take my code here","start":912160,"end":916880,"dur":4720},{"text":"which basically just uses Lang chain to","start":914600,"end":919360,"dur":4760},{"text":"connect to my quadrant Vector store get","start":916880,"end":922399,"dur":5519},{"text":"all of the vector IDs where the metadata","start":919360,"end":925560,"dur":6200},{"text":"file ID is equal to the ID of the file","start":922399,"end":927680,"dur":5281},{"text":"I'm currently ingesting and then it just","start":925560,"end":929839,"dur":4279},{"text":"deletes those vectors so basically we","start":927680,"end":931680,"dur":4000},{"text":"clear everything that's currently in the","start":929839,"end":934120,"dur":4281},{"text":"vector database for this file so that we","start":931680,"end":935920,"dur":4240},{"text":"can reinsert it and make sure that we","start":934120,"end":938480,"dur":4360},{"text":"have zero duplicates that is so","start":935920,"end":940160,"dur":4240},{"text":"important because you don't want","start":938480,"end":942440,"dur":3960},{"text":"different versions of your file existing","start":940160,"end":944040,"dur":3880},{"text":"at the same time in your knowledge base","start":942440,"end":945839,"dur":3399},{"text":"that will confuse the heck out of your","start":944040,"end":949440,"dur":5400},{"text":"llm and so this is a very important","start":945839,"end":951319,"dur":5480},{"text":"step and so I'll run this as well and","start":949440,"end":952839,"dur":3399},{"text":"that's going to delete everything so I","start":951319,"end":955240,"dur":3921},{"text":"can even go back to quadrant here go to","start":952839,"end":957240,"dur":4401},{"text":"my Collections and you can see now that","start":955240,"end":959199,"dur":3959},{"text":"this number was nine when I first showed","start":957240,"end":960759,"dur":3519},{"text":"this quadrant dashboard and now it is","start":959199,"end":963399,"dur":4200},{"text":"zero but it's going to go back up to 9","start":960759,"end":965560,"dur":4801},{"text":"when I finish this workflow so next up","start":963399,"end":967160,"dur":3761},{"text":"we're going to download this Google","start":965560,"end":968120,"dur":2560},{"text":"drive","start":967160,"end":971399,"dur":4239},{"text":"file nice and simple then we're going","start":968120,"end":973600,"dur":5480},{"text":"to extract the text from it and so this","start":971399,"end":975759,"dur":4360},{"text":"it doesn't matter if it's a PDF a CSP a","start":973600,"end":978279,"dur":4679},{"text":"Google doc it'll take the file and get","start":975759,"end":981000,"dur":5241},{"text":"the raw text from it and then we're","start":978279,"end":983279,"dur":5000},{"text":"going to insert it into our quadrant","start":981000,"end":985120,"dur":4120},{"text":"Vector store and so now I'm going to run","start":983279,"end":986959,"dur":3680},{"text":"test step here and we're going to go","start":985120,"end":989279,"dur":4159},{"text":"back to the UI after it's done doing","start":986959,"end":990959,"dur":4000},{"text":"these insertions you can see here nine","start":989279,"end":993920,"dur":4641},{"text":"items because it chunked up my document","start":990959,"end":995560,"dur":4601},{"text":"so we go back here and I'll refresh it","start":993920,"end":998000,"dur":4080},{"text":"right now it's zero I'll refresh and","start":995560,"end":999920,"dur":4360},{"text":"there we go boom we're back up to nine","start":998000,"end":1001519,"dur":3519},{"text":"chunks and the reason there's so many","start":999920,"end":1004120,"dur":4200},{"text":"chunks for such a small document is","start":1001519,"end":1006319,"dur":4800},{"text":"because if we go to my chunk size here","start":1004120,"end":1009160,"dur":5040},{"text":"in my recursive character text splitter","start":1006319,"end":1011040,"dur":4721},{"text":"I have a chunk size of 100 so every","start":1009160,"end":1013040,"dur":3880},{"text":"single time I put in a document it's","start":1011040,"end":1015399,"dur":4359},{"text":"going to get split up into 100 character","start":1013040,"end":1017519,"dur":4479},{"text":"chunks so I want to keep it small just","start":1015399,"end":1020120,"dur":4721},{"text":"because I'm running llama 3 .1 locally I","start":1017519,"end":1021639,"dur":4120},{"text":"don't have the most powerful computer","start":1020120,"end":1023440,"dur":3320},{"text":"and so I want my prompts to be small so","start":1021639,"end":1025760,"dur":4121},{"text":"I'm keeping my context lower by having","start":1023440,"end":1027839,"dur":4399},{"text":"smaller chunk sizes and not returning a","start":1025760,"end":1030600,"dur":4840},{"text":"lot of documents when I perform Rag and","start":1027839,"end":1032439,"dur":4600},{"text":"so the other thing that I wanted to show","start":1030599.9999999999,"end":1035078.9999999999,"dur":4479},{"text":"really quickly here is my document data","start":1032439.0000000001,"end":1037600.0000000001,"dur":5161},{"text":"loader and so or my default data loader","start":1035079,"end":1039799,"dur":4720},{"text":"I'm adding two pieces of metadata here","start":1037599.9999999999,"end":1042239.9999999999,"dur":4640},{"text":"the file ID and the folder ID the more","start":1039799,"end":1044079,"dur":4280},{"text":"important one right here is the file ID","start":1042240,"end":1046959,"dur":4719},{"text":"because that is how I know that a vector","start":1044079,"end":1049919,"dur":5840},{"text":"is tied to a specific document so I use","start":1046959.0000000001,"end":1052080,"dur":5121},{"text":"that in that other step right here to","start":1049919,"end":1054640,"dur":4721},{"text":"delete the old document vectors before I","start":1052080,"end":1056080,"dur":4000},{"text":"insert the new one so that's how I make","start":1054640,"end":1058000,"dur":3360},{"text":"that connection there so that's kind of","start":1056080,"end":1059480,"dur":3400},{"text":"the most in-depth part of this","start":1058000,"end":1061080,"dur":3080},{"text":"walkthrough is how that all works and","start":1059480,"end":1062840,"dur":3360},{"text":"having this custom code here but just","start":1061080,"end":1064559,"dur":3479},{"text":"know that this is so important so","start":1062840,"end":1066160,"dur":3320},{"text":"just take this from me I hope that it","start":1064559,"end":1068360,"dur":3801},{"text":"makes sense to an extent I spent a lot","start":1066160,"end":1070960,"dur":4800},{"text":"of time making this work for you so","start":1068360,"end":1073200,"dur":4840},{"text":"yeah with that is everything we've","start":1070960,"end":1075320,"dur":4360},{"text":"got our agent fully set up everything","start":1073200,"end":1078159,"dur":4959},{"text":"ingested in we have the document","start":1075320,"end":1079799,"dur":4479},{"text":"currently in the knowledge base CU I ran","start":1078159,"end":1082039,"dur":3880},{"text":"through that step by step and so now we","start":1079799,"end":1083679,"dur":3880},{"text":"can go ahead and test this thing so I'm","start":1082039,"end":1085280,"dur":3241},{"text":"going to go to the chat widget here","start":1083679,"end":1086840,"dur":3161},{"text":"actually I'm going to save it first and","start":1085280,"end":1088400,"dur":3120},{"text":"then go to the chat widget and then I'll","start":1086840,"end":1089880,"dur":3040},{"text":"ask it a question that it can only","start":1088400,"end":1091919,"dur":3519},{"text":"answer if it actually has the document","start":1089880,"end":1093640,"dur":3760},{"text":"in the knowledge base and can retrieve","start":1091919,"end":1096640,"dur":4721},{"text":"it so I'll say what is the ad campaign","start":1093640,"end":1099320,"dur":5680},{"text":"focusing on and because this is llama","start":1096640,"end":1101640,"dur":5000},{"text":"3.1 running locally it's going to","start":1099320,"end":1102880,"dur":3560},{"text":"actually take a little bit to get a","start":1101640,"end":1104799,"dur":3159},{"text":"response because I don't have the BPS","start":1102880,"end":1106840,"dur":3960},{"text":"computer so I'm going to pause and come","start":1104799,"end":1109000,"dur":4201},{"text":"back when it has an answer for me all","start":1106840,"end":1111480,"dur":4640},{"text":"right so we got an answer from llama 3.1","start":1109000,"end":1113440,"dur":4440},{"text":"and this is looking pretty good it's a","start":1111480,"end":1115039,"dur":3559},{"text":"little bit awkward at the start of the","start":1113440,"end":1117000,"dur":3560},{"text":"response here but this is just the","start":1115039,"end":1119240,"dur":4201},{"text":"raw output without any instructions from","start":1117000,"end":1121080,"dur":4080},{"text":"me to the model on how to format a","start":1119240,"end":1123039,"dur":3799},{"text":"response and so you can very easily","start":1121080,"end":1125200,"dur":4120},{"text":"fix this by just adding to the system","start":1123039,"end":1127720,"dur":4681},{"text":"prompt for the llm and telling it how to","start":1125200,"end":1129360,"dur":4160},{"text":"respond with the information it's given","start":1127720,"end":1131559,"dur":3839},{"text":"from rag but overall it does have the","start":1129360,"end":1132880,"dur":3520},{"text":"right answer and it's talking about","start":1131559,"end":1134840,"dur":3281},{"text":"robotic pets which obviously it is only","start":1132880,"end":1136720,"dur":3840},{"text":"going to get that if it's using regag on","start":1134840,"end":1138280,"dur":3440},{"text":"the meaning notes document that I have","start":1136720,"end":1140200,"dur":3480},{"text":"uploaded through my Google Drive so this","start":1138280,"end":1142919,"dur":4639},{"text":"is working absolutely beautifully now I","start":1140200,"end":1144440,"dur":4240},{"text":"would probably want to do a lot more","start":1142919,"end":1146960,"dur":4041.0000000000005},{"text":"testing with this whole setup U but just","start":1144440,"end":1148280,"dur":3840},{"text":"to keep things simple right now I'm","start":1146960,"end":1149840,"dur":2880},{"text":"going to leave it at this as a simple","start":1148280,"end":1151760,"dur":3480},{"text":"example but yeah I would encourage","start":1149840,"end":1153480,"dur":3640},{"text":"you to just take this forward keep","start":1151760,"end":1156120,"dur":4360},{"text":"working on this agent and yeah it's","start":1153480,"end":1158559,"dur":5079},{"text":"fully local it is just a beautiful","start":1156120,"end":1160440,"dur":4320},{"text":"thing so I hope that this whole local AI","start":1158559,"end":1162960,"dur":4401},{"text":"setup is just as cool for you as it is","start":1160440,"end":1164640,"dur":4200},{"text":"for me because I have been having a","start":1162960,"end":1166960,"dur":4000},{"text":"blast with this and I will continue to","start":1164640,"end":1168880,"dur":4240},{"text":"as I keep expanding on it so just","start":1166960,"end":1171039,"dur":4078.9999999999995},{"text":"as I promised in the start of the video","start":1168880,"end":1172400,"dur":3520},{"text":"I want to talk a little bit about how","start":1171039,"end":1173880,"dur":2841},{"text":"I'm planning on expanding this in the","start":1172400,"end":1175840,"dur":3440},{"text":"future to make it even better cuz here's","start":1173880,"end":1178000,"dur":4120},{"text":"the thing this whole stack that I showed","start":1175840,"end":1179720,"dur":3880},{"text":"here is a really good starting point but","start":1178000,"end":1181159,"dur":3159},{"text":"there's some things I want to add on to","start":1179720,"end":1183039,"dur":3319},{"text":"it as well to make it even more robust","start":1181159,"end":1184840,"dur":3681},{"text":"things like redis for caching or a","start":1183039,"end":1187000,"dur":3961},{"text":"self-hosted super base instead of the","start":1184840,"end":1188799,"dur":3959},{"text":"vanilla postgress CU then it can handle","start":1187000,"end":1190960,"dur":3960},{"text":"things like authentication as well maybe","start":1188799,"end":1192840,"dur":4041.0000000000005},{"text":"even turning this into a whole local AI","start":1190960,"end":1194640,"dur":3680},{"text":"Tech stack that would even include","start":1192840,"end":1196240,"dur":3400},{"text":"things like the front end as well or","start":1194640,"end":1198880,"dur":4240},{"text":"maybe baking in best practices for red","start":1196240,"end":1201480,"dur":5240},{"text":"and llms or na end workflows for that to","start":1198880,"end":1203000,"dur":4120},{"text":"make this more of like a template as","start":1201480,"end":1204760,"dur":3280},{"text":"well to actually make it really","start":1203000,"end":1207280,"dur":4280},{"text":"easy to get started with local AI so I","start":1204760,"end":1208679,"dur":3919},{"text":"hope that you're excited about that if","start":1207280,"end":1210799,"dur":3519},{"text":"you are or if you found this video just","start":1208679,"end":1212360,"dur":3681},{"text":"helpful in general getting you set up","start":1210799,"end":1214400,"dur":3601},{"text":"with your local AI Tech stack I would","start":1212360,"end":1216480,"dur":4120},{"text":"really appreciate a like and a subscribe","start":1214400,"end":1218480,"dur":4080},{"text":"and with that I will see you in the next","start":1216480,"end":1220919,"dur":4439},{"text":"video","start":1218480,"end":1220919,"dur":2439}],"vocabulary":"AI, LLMs, RAG, Ollama, Qdrant, Postgres, n8n, coleam00, local AI, Llama 3.1, 8b, 70b, GitHub, Docker, laptop, 3070, i7-12700H, chatbot ","summary":"This video demonstrates setting up a local AI infrastructure using open-source tools.  The creator shows how to install and configure a package containing LLMs (like Llama 3.1), a vector database (Qdrant), a SQL database (Postgres), and workflow automation (n8n).  Key improvements are made to the Docker Compose file, including exposing the Postgres port and adding an embedding model to Ollama, enabling its use in a RAG (Retrieval Augmented Generation) AI agent within n8n.  The video also details how to ingest files from Google Drive into the knowledge base using n8n and Langchain.  The setup enables interaction with a chatbot powered by the local AI stack.\n","punctuatedText":"Have you ever wished for a single package that easily installs everything you need for local AI? Well, I have good news for you today, because I have exactly what you're looking for. I've actually never been so excited to make a video on something before today. I'm going to show you an incredible package for local AI, developed by the n8n team. This package has it all:\n\n*   Llama for LLMs\n*   Qdrant for the vector database\n*   PostgreSQL for the SQL database\n*   And n8n to tie it all together with workflow automations.\n\nThis is absolutely incredible, and I'm going to show you how to set it up in just minutes.  Then, I'll even show you how to extend it to make it better and use it to create a full RAG AI agent within n8n. Stick around, because I have a lot of value for you today.\n\nRunning your own AI infrastructure is the way of the future, especially because of how accessible it's becoming. And because open-source models like Llama are reaching a point where they're powerful enough to compete with closed-source models like GPT and Claude, now is the time to jump on board.  What I'm about to show you is an excellent starting point for doing so. At the end of this video, I'll even talk about how I'm going to extend this package in the near future, just for you, to make it even better.\n\nAll right, so here we are in the GitHub repository for the self-hosted AI starter kit by n8n. This repo is really basic, and I love it. There are basically just two files we need to care about:\n\n1.  An environment variable file where we'll set credentials for things like PostgreSQL.\n2.  A Docker Compose YAML file where we'll bring in everything together‚ÄîPostgreSQL, Qdrant, and Ollama‚Äîto have a single package for our local AI.\n\nThe README has instructions for installing everything yourself, but honestly, it's quite lacking. There are a couple of gaps I want to fill in here with ways to extend it to make it truly what you need. So, I'll go through that now, and we'll actually get this installed on our computer.\n\nBefore you begin, there are a couple of dependencies: Git and Docker. I'd recommend installing GitHub Desktop and Docker Desktop as well, because Docker Compose is included, and we need that to bring everything together for one package.  With that, we can go ahead and get started downloading this on our computer.  First, copy the \"git clone\" command and the repository URL into a terminal, then paste the command.  . I've already cloned this repository, which is why I'm getting this error message.  You will download this code to your computer.  Then, change your directory to the newly pulled repository.  Now, we can edit the files in any editor of our choice. I like using Visual Studio Code. If you also use VS Code, type \"code .\" to open everything in Visual Studio Code.\n\nThe official instructions in the README would tell you to run everything with the `docker-compose up` command at this point.  However, this is not the correct next step.  We need to edit a few things in the code to customize it for our use case.  This starts with the `.env` file.\n\nGo into your `.env` file.  I've created an example file, `.env.example`, because I've already set up my credentials. Set up your PostgreSQL username and password, the database name, and a couple of n8n secrets.  These secrets can be anything, but make sure they are long alphanumeric strings.\n\nNext, open the `docker-compose.yml` file.  Here, I want to make a few additions to fill in some gaps.  The original `docker-compose.yml` file was missing a few things.\n\nFirst, the PostgreSQL container does not expose its port by default.  This means you can't use PostgreSQL as your database in an n8n workflow.  n8n likely uses PostgreSQL internally, which is why it's initially set up that way. However, we need to be able to access PostgreSQL for our chat memory and agents.\n\nTo do this, go down to the PostgreSQL service in the `docker-compose.yml` file and add these two lines:\n\n```yaml\nports:\n  - \"5432:5432\"\n```\n\nThis maps port 5432 on your host machine to port 5432 inside the container. This allows you to access PostgreSQL at `localhost:5432`. This is crucial for accessing it within an n8n workflow, which we will use later when building the RAG AI agent.\n\nNext, we want to use Ollama for our embeddings and vector database.  The base command for initializing Ollama is `sleep 3; ollama pull llama 3 1 8b`. This is available by default, but I've added another line to pull one of the Ollama embedding models.  .  We need this to use Ollama for our RAG.  I've also added this crucial line.  That's literally everything you need to change in the code to get this working.  I'll include a link in the video description to my version, where you can directly download all the customizations we just covered.\n\nWith that, we can start using Docker Compose. The installation instructions in the README are helpful, because there's a slightly different Docker Compose command based on your architecture.\n\nIf you have an Nvidia GPU, follow these more complicated instructions to run with an Nvidia profile.  Alternatively, you can use these instructions. If you're a Mac user, use this command.  For everyone else, even though I have an Nvidia GPU, I'll use a simple CPU-based Docker Compose command.\n\nCopy this command, go to your terminal, and paste it.  In my case, I've already created all the containers, so it will run very quickly.  In your case, it will need to pull images for Ollama, Postgres, n8n, and Qdrant, and then start them all. This might take a little while, because I also need to pull Llama 3.1 for the Ollama container.  My setup is already complete, so this will be a quick walkthrough for you.\n\nYou can see all the different running containers, in various colors, that set up each service.  For example, it pulled Llama 3.1 and the embedding model I chose from Ollama.  The setup is basically complete.  I will pause here and return when everything is ready.\n\nAll right, everything is ready to go.  Now, I'll take you into Docker to see everything running live. Open your Docker desktop. You'll see one entry for the self-hosted AI starter kit. Click the button on the left to expand it.  You'll see all the containers currently running or that have run for the setup‚Äîfour containers in total‚Äîrunning one of our different local AI services. You can click into each one.  .  This is cool because we can see the output of each container and even go to the exec tab to run Linux commands within each of these containers.  So, you can actually do things in real time without having to restart the containers.  You can go into the Postgres container and run commands to query your tables and things.  You can go into the Ollama container and pull in real-time models.  For example, if I want to go to exec here, I can do `ollama pull llama-3.1-70b`.  I can pull models in real-time and have them updated and available to me in n8n without having to restart anything, which is super cool.\n\nAlright, now for the fun part: using the local infrastructure to create a fully local RAG AI agent within n8n.  To access your self-hosted n8n, go to localhost:5678.  You can find this URL in the Docker logs for your n8n container or in the README in the GitHub repository you cloned.  This workflow uses Postgres for chat memory, Qdrant for RAG, and Ollama for the LLM and embedding model.  This is a complete RAG AI agent that I've already built; I'm not building it from scratch to keep this a quicker walkthrough.  I will still walk through every step so you understand it and can use it.  I will include this workflow in the description as a link so you can use it in your own n8n instance.\n\nWith that, let's get started.  This workflow has two parts: the agent itself, with the chat interaction, and the workflow pulling files from Google Drive to your knowledge base using Qdrant.  First, the agent. The chat widget lets you interact with the agent.  Then, I'll quickly show the pipeline to bring files from a Google Drive folder into your knowledge base.\n\nThe trigger, the chat input, feeds directly into the AI agent, connecting all the local components.  First, we have the Ollama chat model setup referencing the Llama 3 1.8 billion parameter model. If you want to use an agent (AMA) within a container, you can use any Ollama LLM.  .  Setting up is straightforward.\n\nFor credentials:\n\n*   Base URL:  Use `http://host.doer.internal` instead of `localhost`.  This is crucial.\n*   Port: 11434 (check Docker Compose file or AMA container logs).\n\nMemory: PostgreSQL\n\n*   Table name: Any name you choose.  n8n will automatically create the table in PostgreSQL.\n*   Retrieves session ID from the previous node.\n*   Credentials:  Based on your `.env` file values.  Host: `host.doer.internal`, database name, user, and password are all defined there.\n*   Port: 5432\n\nRAG Tool: Vector Store\n\n*   Attach a vector store tool to the agent.\n*   Use Qdrant vector store.\n*   Retrieves documents based on agent queries.\n*   Credentials: API key (likely in your n8n instance).\n*   URL: `http://host.doer.internal:6333`\n*   Port: 6333 (check Docker Compose file or Qdrant logs).\n\nLocal Qdrant Dashboard\n\nBy accessing `http://localhost:6333/dashboard` in your browser, you can view the Qdrant dashboard.  This shows collections and your knowledge base; the vectors you have can be visualized.  .  You can see all your different vectors.  This is a document I've inserted for testing.  You can see all the metadata and contents of each chunk. It's very cool.  We'll return to this later, but you have excellent visibility into your Qdrant instance.  You can run queries, get collections, delete vectors, or search.  Hosting Qdrant is fantastic.\n\nWe're using Qdrant as our vector store.  We're using Ollama for embeddings, using the embedding model I added to the Docker Compose file. We're using Llama 3.1 again to parse the responses we get from RAG when we do lookups.  That's everything for our agent.  We'll test it later.  First, I want to show you the workflow for ingesting files into our knowledge base.\n\nThe workflow has two triggers.  Whenever a file is created in a specific Google Drive folder, or updated in that same folder, this pipeline downloads the file and places it into our Qdrant vector database, running locally.  The folder is \"Meeting Notes\" in my Google Drive.  The document for testing is \"Fake Meeting Notes.\"  I generated something silly about a company selling robotic pets and an AI startup.  We'll use this document for RAG.  For simplicity, I'm not using multiple documents, but Qdrant can handle that.  For now, we're using this single document.\n\nI'll walk you through the ingestion flow step-by-step.  First, we'll fetch a test event‚Äîthe creation of the \"Meeting Note\" file.  Then, we'll feed that into a node that extracts key information, including the file ID and folder ID.\n\nNext, there's a crucial step.  Many n8n RAG tutorials on YouTube miss this.  I'll skip to the end, this really quickly.  . Whether this is a super basic Qdrant setup, it doesn't matter.  When you have this inserter, it's not an upsert; it's just an insert.  This means if you reinsert a document, you'll actually have duplicate vectors for that document.\n\nSo, if I update a document in Google Drive and it reinserts the vectors into my Qdrant vector database, I'll have the old vectors from when I first ingested the document, and then new vectors from when I updated the file. It doesn't remove the old files or update the vectors in place. This is crucial to remember.\n\nI'm providing significant value here by including this node.  It's custom code because there's no way to do it without code in n8n.  However, it's straightforward because you can copy this from me.  I'll link this workflow in the description, so you can download it and use it in your own n8n setup.  My code uses LangChain to connect to my Qdrant vector store.  It gets all the vector IDs where the metadata file ID equals the ID of the file I'm currently ingesting.  Then, it deletes those vectors.\n\nEssentially, it clears everything currently in the vector database for this file, so we can reinsert it and ensure zero duplicates.  This is critical because you don't want different versions of your file in your knowledge base simultaneously.  That will confuse your LLM.  This is a very important step.\n\nI'll run this now, and it will delete everything.  I can even go back to Qdrant, go to my Collections, and see that the count was nine when I first showed the Qdrant dashboard, and now it's zero.  It will go back up to nine when I finish this workflow.\n\nNext, we'll download this Google Drive file.  Simple enough. Then, we'll extract the text from it.  Whether it's a PDF, a .CSP file, or a Google Doc, it will extract the raw text. Then, we'll insert it into our Qdrant vector store.\n\nNow, I'm going to run the test step, and we'll go back to the UI after the insertions are complete.  You can see nine items, because my document was chunked up. Let's go back and refresh. It's zero. Refresh again, and there we go! We're back up to nine chunks.  The reason there are so many chunks for such a small document is because...  The rest of the transcript is truncated. My recursive character text splitter has a chunk size of 100.  .  Every time I add a document, it's split into 100-character chunks. I'm keeping it small because I'm running Llama 3.1 locally on a less powerful computer.  Smaller prompts mean lower context, and fewer documents returned by RAG.\n\nThe other crucial part is my document data loader (or default data loader).  I'm adding two pieces of metadata: file ID and folder ID. The file ID is more important because it links a vector to a specific document.  I use this ID in a previous step to delete old document vectors before inserting new ones.  This maintains the connection. This custom code is a critical part of the walkthrough.\n\nThis setup is important.  I've put a lot of time into making this work for you.  I hope it makes sense to some extent.\n\nThe agent is fully set up, and all documents are ingested into the knowledge base.  I've run through the steps, and the document is now in the knowledge base.\n\nNow, I can test it.  First, I'll save the setup and then go to the chat widget. I'll ask a question that can only be answered if the document is in the knowledge base: \"What is the ad campaign focusing on?\"\n\nBecause I'm running Llama 3.1 locally, the response will take a while. I'll pause the video and return when I get a response.\n\n(Video pause)\n\nHere's the response from Llama 3.1. It looks good, but the beginning is a little awkward.  This is the raw output without any formatting instructions from me.  Adding instructions to the LLM system prompt on how to format the response would easily fix this.  The answer is correct; it mentions robotic pets.  This is only possible if RAG is retrieving information from the uploaded \"meaning notes\" document.\n\nThis is working beautifully.  I would probably want to do a lot more testing with this whole setup, but to keep things simple right now, I'm going to leave it at this as a simple example.  .  But yeah, I would encourage you to take this forward and keep working on this agent.  Yeah, it's fully local; it's a beautiful thing.  So I hope that this whole local AI setup is just as cool for you as it is for me, because I've been having a blast with it, and I will continue to as I keep expanding on it.\n\nSo, just as I promised at the start of the video, I want to talk a little bit about how I'm planning on expanding this in the future to make it even better.  Because here's the thing: this whole stack that I showed here is a really good starting point, but there are some things I want to add on to it to make it even more robust. Things like Redis for caching or a self-hosted Superbase instead of the vanilla Postgres.  Then, it can handle things like authentication as well. Maybe even turning this into a whole local AI tech stack that would even include things like the front end as well, or maybe baking in best practices for LLMs or n8n workflows to make this more of a template to actually make it really easy to get started with local AI.\n\nI hope that you're excited about that. If you are, or if you found this video just helpful in general for getting you set up with your local AI tech stack, I would really appreciate a like and a subscribe.  With that, I will see you in the next video.\n"},"highlights":[]}