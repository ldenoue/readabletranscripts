{"videoId":"S53BanCP14c","title":"Getting Started with Groq API | Making Near Real Time Chatting with LLMs Possible","description":"Let's have a look at the Groq API that offers access to their Language Processing Units (LPUs) for free (for a limited time). The API can provide close to 500 tokens per second with Mixtral-8x7B. Enjoy :) \n\nü¶æ Discord: https://discord.com/invite/t4eYQRUcXB\n‚òï Buy me a Coffee: https://ko-fi.com/promptengineering\n|üî¥ Patreon: https://www.patreon.com/PromptEngineering\nüíºConsulting: https://calendly.com/engineerprompt/consulting-call\nüìß Business Contact: engineerprompt@gmail.com\nBecome Member: http://tinyurl.com/y5h28s6h\n\nüíª Pre-configured localGPT VM: https://bit.ly/localGPT (use Code: PromptEngineering for 50% off).  \n\n\nLINKS:\nSignup for API Access: https://groq.com/\nColab Notebook: http://tinyurl.com/2nxdv2m8\nStreamlit Chatbot: http://tinyurl.com/3f485knj\n\n\nTIMESTAMPS:\n[00:00] Introduction \n[00:34] How to Access the Groq API?\n[01:06]  API Playground\n[03:15] Getting Started with Groq API\n[05:33] Near real-time speed\n[07:24] Advanced API Features: Streaming and Stop Sequences\n[11:02] Building a Chatbot with Streamlit and Grok API\n\n\n\nAll Interesting Videos:\nEverything LangChain: https://www.youtube.com/playlist?list=PLVEEucA9MYhOu89CX8H3MBZqayTbcCTMr\n\nEverything LLM: https://youtube.com/playlist?list=PLVEEucA9MYhNF5-zeb4Iw2Nl1OKTH-Txw\n\nEverything Midjourney: https://youtube.com/playlist?list=PLVEEucA9MYhMdrdHZtFeEebl20LPkaSmw\n\nAI Image Generation: https://youtube.com/playlist?list=PLVEEucA9MYhPVgYazU5hx6emMXtargd4z","viewCount":"36549","duration":979,"publishDate":"2024-02-29T03:15:02-08:00","thumbnail":"https://img.youtube.com/vi/S53BanCP14c/mqdefault.jpg","translationLanguages":{"ab":"Abkhazian","aa":"Afar","af":"Afrikaans","ak":"Akan","sq":"Albanian","am":"Amharic","ar":"Arabic","hy":"Armenian","as":"Assamese","ay":"Aymara","az":"Azerbaijani","bn":"Bangla","ba":"Bashkir","eu":"Basque","be":"Belarusian","bho":"Bhojpuri","bs":"Bosnian","br":"Breton","bg":"Bulgarian","my":"Burmese","ca":"Catalan","ceb":"Cebuano","zh-Hans":"Chinese (Simplified)","zh-Hant":"Chinese (Traditional)","co":"Corsican","hr":"Croatian","cs":"Czech","da":"Danish","dv":"Divehi","nl":"Dutch","dz":"Dzongkha","en":"English","eo":"Esperanto","et":"Estonian","ee":"Ewe","fo":"Faroese","fj":"Fijian","fil":"Filipino","fi":"Finnish","fr":"French","gaa":"Ga","gl":"Galician","lg":"Ganda","ka":"Georgian","de":"German","el":"Greek","gn":"Guarani","gu":"Gujarati","ht":"Haitian Creole","ha":"Hausa","haw":"Hawaiian","iw":"Hebrew","hi":"Hindi","hmn":"Hmong","hu":"Hungarian","is":"Icelandic","ig":"Igbo","id":"Indonesian","iu":"Inuktitut","ga":"Irish","it":"Italian","ja":"Japanese","jv":"Javanese","kl":"Kalaallisut","kn":"Kannada","kk":"Kazakh","kha":"Khasi","km":"Khmer","rw":"Kinyarwanda","ko":"Korean","kri":"Krio","ku":"Kurdish","ky":"Kyrgyz","lo":"Lao","la":"Latin","lv":"Latvian","ln":"Lingala","lt":"Lithuanian","lua":"Luba-Lulua","luo":"Luo","lb":"Luxembourgish","mk":"Macedonian","mg":"Malagasy","ms":"Malay","ml":"Malayalam","mt":"Maltese","gv":"Manx","mi":"MƒÅori","mr":"Marathi","mn":"Mongolian","mfe":"Morisyen","ne":"Nepali","new":"Newari","nso":"Northern Sotho","no":"Norwegian","ny":"Nyanja","oc":"Occitan","or":"Odia","om":"Oromo","os":"Ossetic","pam":"Pampanga","ps":"Pashto","fa":"Persian","pl":"Polish","pt":"Portuguese","pt-PT":"Portuguese (Portugal)","pa":"Punjabi","qu":"Quechua","ro":"Romanian","rn":"Rundi","ru":"Russian","sm":"Samoan","sg":"Sango","sa":"Sanskrit","gd":"Scottish Gaelic","sr":"Serbian","crs":"Seselwa Creole French","sn":"Shona","sd":"Sindhi","si":"Sinhala","sk":"Slovak","sl":"Slovenian","so":"Somali","st":"Southern Sotho","es":"Spanish","su":"Sundanese","sw":"Swahili","ss":"Swati","sv":"Swedish","tg":"Tajik","ta":"Tamil","tt":"Tatar","te":"Telugu","th":"Thai","bo":"Tibetan","ti":"Tigrinya","to":"Tongan","ts":"Tsonga","tn":"Tswana","tum":"Tumbuka","tr":"Turkish","tk":"Turkmen","uk":"Ukrainian","ur":"Urdu","ug":"Uyghur","uz":"Uzbek","ve":"Venda","vi":"Vietnamese","war":"Waray","cy":"Welsh","fy":"Western Frisian","wo":"Wolof","xh":"Xhosa","yi":"Yiddish","yo":"Yoruba","zu":"Zulu"},"defaultLanguage":"en","en":{"chunks":[{"text":"you probably remember Gro the company","start":40,"end":4400,"dur":4360},{"text":"that is building language processing","start":2720,"end":6919,"dur":4199},{"text":"units for fast inference of","start":4400,"end":10960,"dur":6560},{"text":"llms well they just started rolling out","start":6919,"end":13799,"dur":6880},{"text":"API access to developers they were","start":10960,"end":16279,"dur":5319},{"text":"claiming nearly 500 tokens per second","start":13799,"end":20080,"dur":6281},{"text":"for mix Moe and in my testing that seems","start":16279,"end":22560,"dur":6281},{"text":"to be true in this video I'll show you","start":20080,"end":25960,"dur":5880},{"text":"how to access the API for free and then","start":22560,"end":27840,"dur":5280},{"text":"I'll show you a couple of example use","start":25960,"end":29599,"dur":3639},{"text":"cases in which we're going to build a","start":27840,"end":30840,"dur":3000},{"text":"chat box","start":29599,"end":34120,"dur":4521},{"text":"and this thing is crazy fast more on","start":30840,"end":36640,"dur":5800},{"text":"that later in the video in order to get","start":34120,"end":38600,"dur":4480},{"text":"access to the API you'll need to go to","start":36640,"end":42960,"dur":6320},{"text":"gro.com log in with your email or in my","start":38600,"end":45600,"dur":7000},{"text":"case I logged in with my Google account","start":42960,"end":48399,"dur":5439},{"text":"apart from the API they also opened up","start":45600,"end":51160,"dur":5560},{"text":"their playground where you can test two","start":48399,"end":53800,"dur":5401},{"text":"models currently one is the Lama 270","start":51160,"end":56840,"dur":5680},{"text":"Bill model and the other one is the mixe","start":53800,"end":59920,"dur":6120},{"text":"model they also are providing really","start":56840,"end":62399,"dur":5559},{"text":"detailed documentation more on this in a","start":59920,"end":64680,"dur":4760},{"text":"little bit and you will be able to","start":62399,"end":67920,"dur":5521},{"text":"create your API keys in here before","start":64680.00000000001,"end":70280,"dur":5600},{"text":"working with the API let's have a quick","start":67920,"end":72119,"dur":4199},{"text":"look at the playground you can provide","start":70280,"end":75640,"dur":5360},{"text":"your system message here then user input","start":72119,"end":77439,"dur":5320},{"text":"here you have two different options for","start":75640,"end":79680,"dur":4040},{"text":"the model so for our experiments we're","start":77439,"end":81560,"dur":4121},{"text":"going to be using the","start":79680,"end":84759,"dur":5079},{"text":"mixe okay so my system messages you are","start":81560,"end":88640,"dur":7080},{"text":"a helpful assistant answer as John Snow","start":84759,"end":91240,"dur":6481},{"text":"the user input is explain the importance","start":88640,"end":95159,"dur":6519},{"text":"of low latency llms since we are testing","start":91240,"end":98119,"dur":6879},{"text":"the speed of groc API so I think this is","start":95159,"end":100840,"dur":5681},{"text":"an appropriate question and then you can","start":98119,"end":103240,"dur":5121},{"text":"also set other parameters this will","start":100840,"end":105360,"dur":4520},{"text":"control the behavior of the model so you","start":103240,"end":107360,"dur":4120},{"text":"can set the temperature maximum new","start":105360,"end":109960,"dur":4600},{"text":"tokens that is supposed to generate also","start":107360,"end":112799,"dur":5439},{"text":"top p and if you want to include any","start":109960,"end":115680,"dur":5720},{"text":"stop sequence hit submit and this is","start":112799,"end":117439,"dur":4640},{"text":"realtime speed that you're going to get","start":115680,"end":119840,"dur":4160},{"text":"from the API let's have a quick look","start":117439,"end":122880,"dur":5441},{"text":"look at the response well greetings as","start":119840,"end":126280,"dur":6440},{"text":"John known I am not an expert in low","start":122880,"end":129520,"dur":6640},{"text":"latency large language models but I can","start":126280,"end":131760,"dur":5480},{"text":"certainly try to explain their","start":129520.00000000001,"end":134840,"dur":5320},{"text":"importance in a simple Manner and then","start":131760,"end":138000,"dur":6240},{"text":"it goes on to generate a response in","start":134840,"end":140400,"dur":5560},{"text":"this video we are not really interested","start":138000,"end":142959,"dur":4959},{"text":"in the accuracy of the responses we will","start":140400,"end":146120,"dur":5720},{"text":"be only concerned about the speed so","start":142959,"end":147680,"dur":4721},{"text":"that's why I'm not going to go over the","start":146120,"end":150319,"dur":4199},{"text":"responses that the model generates","start":147680,"end":152800,"dur":5120},{"text":"so let's say you come up with parameters","start":150319,"end":155200,"dur":4881},{"text":"for the Model Behavior after that you","start":152800,"end":157920,"dur":5120},{"text":"can click on this view code button and","start":155200,"end":160720,"dur":5520},{"text":"this will give you python code on how to","start":157920,"end":164360,"dur":6440},{"text":"start calling this API there's also a","start":160720,"end":167239,"dur":6519},{"text":"code available to call the API within","start":164360,"end":170879,"dur":6519},{"text":"JavaScript or even Json next I'm going","start":167239,"end":173680,"dur":6441},{"text":"to walk you through a few examples for","start":170879,"end":176640,"dur":5761},{"text":"that we need to create a new API key so","start":173680,"end":179720,"dur":6040},{"text":"click on create API key provide the AP","start":176640,"end":182959,"dur":6319},{"text":"API key name let's say we're going to","start":179720,"end":186319,"dur":6599},{"text":"call this Croc one and if you hit submit","start":182959,"end":188920,"dur":5961},{"text":"this will show you your API key just","start":186319,"end":191879,"dur":5560},{"text":"copy this API key in a secure location I","start":188920,"end":193720,"dur":4800},{"text":"am going to delete this because I'm","start":191879,"end":196040,"dur":4161},{"text":"going to be using the existing API key","start":193720,"end":198799,"dur":5079},{"text":"that I have so let me first walk you","start":196040,"end":201680,"dur":5640},{"text":"through a basic structure of how working","start":198799,"end":203360,"dur":4561},{"text":"with groc looks like I'll walk you","start":201680,"end":205560,"dur":3880},{"text":"through a Google colab but before that","start":203360,"end":209760,"dur":6400},{"text":"we need to install the package using pip","start":205560,"end":212239,"dur":6679},{"text":"so we're going to use pip install Croc","start":209760,"end":214840,"dur":5080},{"text":"this will install the package for us","start":212239,"end":217360,"dur":5121},{"text":"after that in order to use this package","start":214840,"end":219840,"dur":5000},{"text":"we need to import it we are importing","start":217360,"end":222640,"dur":5280},{"text":"operating system then we are importing","start":219840,"end":225799,"dur":5959},{"text":"the gron package next we need to create","start":222640,"end":228000,"dur":5360},{"text":"a client so this is going to be the","start":225799,"end":229360,"dur":3561},{"text":"grock client that we're going to be","start":228000,"end":232480,"dur":4480},{"text":"using you will need to provide your API","start":229360,"end":235760,"dur":6400},{"text":"key since I'm am using the notebook so","start":232480,"end":238599,"dur":6119},{"text":"I'll have to set the environment","start":235760,"end":240200,"dur":4440},{"text":"variable within the notebook book I'll","start":238599,"end":242840,"dur":4241},{"text":"show you that in a bit once you create","start":240200,"end":246079,"dur":5879},{"text":"the client the rest of the process is","start":242840,"end":250079,"dur":7239},{"text":"very similar to calling the openai API","start":246079,"end":252480,"dur":6401},{"text":"key so we are going to be using the","start":250079,"end":255959,"dur":5880},{"text":"chart completion endpoint we create a","start":252480,"end":259359,"dur":6879},{"text":"new message so here we define the role","start":255959,"end":261440,"dur":5481},{"text":"the role is user so this is directly","start":259358.99999999997,"end":264240,"dur":4881},{"text":"interacting with the model later I'll","start":261440,"end":267000,"dur":5560},{"text":"show you how to define the system","start":264240,"end":269000,"dur":4760},{"text":"role as well so that you can provide a","start":267000,"end":270360,"dur":3360},{"text":"system message","start":269000,"end":273639,"dur":4639},{"text":"next you need to provide your prompt","start":270360,"end":276320,"dur":5960},{"text":"from the user after that you will need","start":273639,"end":278639,"dur":5000},{"text":"to select the model that you want to use","start":276320,"end":281479,"dur":5159},{"text":"in this case we're using the mixt Moe","start":278639,"end":286800,"dur":8161},{"text":"model and once you make the call you can","start":281479,"end":289600,"dur":8121},{"text":"get the response of the model using the","start":286800,"end":292080,"dur":5280},{"text":"choices variable then messages and then","start":289600,"end":295000,"dur":5400},{"text":"the content of the message so that's how","start":292080,"end":297919,"dur":5839},{"text":"the basic structure of the API usage is","start":295000,"end":299759,"dur":4759},{"text":"going to look like here's a Google","start":297919,"end":302320,"dur":4401},{"text":"collab that I'm using this is exactly","start":299759,"end":305000,"dur":5241},{"text":"the same code that walk you through but","start":302320,"end":307600,"dur":5280},{"text":"let's first set an environment variable","start":305000,"end":310360,"dur":5360},{"text":"so you can click on this Secrets button","start":307600,"end":313440,"dur":5840},{"text":"then create or add a new secret here","start":310360,"end":316600,"dur":6240},{"text":"I have provided my grock API key so","start":313440,"end":318960,"dur":5520},{"text":"you'll provide the name of the secret or","start":316600,"end":320759,"dur":4159},{"text":"environment variable and then the","start":318960,"end":323520,"dur":4560},{"text":"corresponding value make sure to toggle","start":320759,"end":326680,"dur":5921},{"text":"this notebook access button so that your","start":323520,"end":329960,"dur":6440},{"text":"notebook can actually see the key that","start":326680,"end":332360,"dur":5680},{"text":"you're using this is exactly the same","start":329960,"end":335919,"dur":5959},{"text":"code that we saw now let me show you the","start":332360,"end":338080,"dur":5720},{"text":"speed of generation in real time so I'm","start":335919,"end":341440,"dur":5521},{"text":"going to click on this button and this","start":338080,"end":344280,"dur":6200},{"text":"will start generating the response Now","start":341440,"end":347479,"dur":6039},{"text":"you kind of feel like it's streaming but","start":344280,"end":350039,"dur":5759},{"text":"this whole thing was generated all at","start":347479,"end":352680,"dur":5201},{"text":"once this is actually crazy fast I","start":350039,"end":355160,"dur":5121},{"text":"haven't seen anything like this before","start":352680,"end":356720,"dur":4040},{"text":"later I'll show you how to actually","start":355160,"end":359120,"dur":3960},{"text":"enable streaming as well although for","start":356720,"end":361360,"dur":4640},{"text":"this you don't really need that now","start":359120,"end":363120,"dur":4000},{"text":"let's look at some other options so how","start":361360,"end":366000,"dur":4640},{"text":"do you add a system message in that case","start":363120,"end":368280,"dur":5160},{"text":"you simply need to provide a new role","start":366000,"end":371240,"dur":5240},{"text":"called system so this is going to become","start":368280,"end":374199,"dur":5919},{"text":"our system message and we are act asking","start":371240,"end":377240,"dur":6000},{"text":"the model to act as JN snow the user","start":374199,"end":380160,"dur":5961},{"text":"message or user role is the same there","start":377240,"end":381800,"dur":4560},{"text":"are quite a few other options that you","start":380160,"end":384840,"dur":4680},{"text":"can also set so for example you can set","start":381800,"end":387479,"dur":5679},{"text":"the temperature this will control the","start":384840,"end":390440,"dur":5600},{"text":"creativity or randomness of your output","start":387479,"end":392599,"dur":5120},{"text":"what is the maximum number of tokens the","start":390440,"end":394479,"dur":4038.9999999999995},{"text":"model can generate or you want the model","start":392599,"end":398000,"dur":5401},{"text":"to generate next is top P this will","start":394479,"end":400360,"dur":5881},{"text":"basically control the sampling mechanism","start":398000,"end":403120,"dur":5120},{"text":"through which it's generating output if","start":400360,"end":405880,"dur":5520},{"text":"you want to use any specific stopwords","start":403120,"end":408520,"dur":5400},{"text":"you can define those here and whether","start":405880,"end":411960,"dur":6080},{"text":"you want to stream the responses or not","start":408520,"end":415039,"dur":6519},{"text":"so if you enable streaming you will need","start":411960,"end":417800,"dur":5840},{"text":"to change the way you get the output and","start":415039,"end":419440,"dur":4401},{"text":"I'm going to show you that later in the","start":417800,"end":422280,"dur":4480},{"text":"video but we are going to use the same","start":419440,"end":424160,"dur":4720},{"text":"prompt explain the importance of low","start":422280,"end":427639,"dur":5359},{"text":"latency llms and let me show you the","start":424160,"end":431360,"dur":7200},{"text":"real time response that we get so here's","start":427639,"end":433840,"dur":6201},{"text":"like the speed at which it was able to","start":431360,"end":435599,"dur":4239},{"text":"generate the output which is pretty","start":433840,"end":439280,"dur":5440},{"text":"amazing and it's also actually sticking","start":435599,"end":442039,"dur":6440},{"text":"to the character that we asked it to so","start":439280,"end":444520,"dur":5240},{"text":"that's pretty nice as well next let's","start":442039,"end":447560,"dur":5521},{"text":"look at streaming responses so here","start":444520,"end":449440,"dur":4920},{"text":"we're using the same structure again","start":447560,"end":451639,"dur":4078.9999999999995},{"text":"again I'm using the previous client that","start":449440,"end":453919,"dur":4479},{"text":"I created so not really creating","start":451639,"end":456919,"dur":5280},{"text":"another client the difference that you","start":453919,"end":460639,"dur":6720},{"text":"will see in here is this so we enabled","start":456919,"end":463039,"dur":6120},{"text":"streaming and now that means that the","start":460639,"end":464639,"dur":4000},{"text":"model is not going to generate the whole","start":463039,"end":467680,"dur":4641},{"text":"response all together but it will create","start":464639,"end":470840,"dur":6201},{"text":"this in chunks so what we need to do is","start":467680,"end":472879,"dur":5199},{"text":"we need to take one of those chunks at a","start":470840,"end":475080,"dur":4240},{"text":"time and show those to the user and","start":472879,"end":477319,"dur":4440},{"text":"that's why the mechanism of printing is","start":475080,"end":479360,"dur":4280},{"text":"different than what we were using before","start":477319,"end":482560,"dur":5241},{"text":"for now let's look at the speed at which","start":479360,"end":485639,"dur":6279},{"text":"it streams data so this is realtime","start":482560,"end":487960,"dur":5400},{"text":"streaming for you as you can see this","start":485639,"end":490199,"dur":4560},{"text":"opens up so many possibilities for","start":487960,"end":492560,"dur":4600},{"text":"example with this you will be able to","start":490199,"end":495440,"dur":5241},{"text":"have speech communication with the llm","start":492560,"end":498479,"dur":5919},{"text":"so you can have a speech to text model","start":495440,"end":501080,"dur":5640},{"text":"which converts your speech into text","start":498479,"end":504280,"dur":5801},{"text":"feeds this through the grock API gets a","start":501080,"end":506879,"dur":5799},{"text":"response in near real time and then you","start":504280,"end":508919,"dur":4639},{"text":" convert it back from text to speech","start":506879,"end":511720,"dur":4841},{"text":"using another model next we will look at","start":508919,"end":514680,"dur":5761},{"text":"how to use stop sequences so in this","start":511720,"end":517959,"dur":6239},{"text":"case we want the model to stop","start":514679.99999999994,"end":521718.99999999994,"dur":7039},{"text":"generation if it encounters a six in its","start":517958.99999999994,"end":524360,"dur":6401},{"text":"output and the prompt is count to 10","start":521719.00000000006,"end":527680,"dur":5961},{"text":"your response must begin with one then","start":524360,"end":529519,"dur":5159},{"text":"we give it example as well like how the","start":527680,"end":531640,"dur":3960},{"text":"generation is supposed to look like the","start":529519,"end":534040,"dur":4521},{"text":"stop sequences are really helpful","start":531640,"end":536200,"dur":4560},{"text":"specifically if you want to interrupt","start":534040,"end":539000,"dur":4960},{"text":"the model generation in the middle so","start":536200,"end":541079,"dur":4879},{"text":"this is actually interrupting the","start":539000,"end":542519,"dur":3519},{"text":"generation although it's supposed to","start":541079,"end":545120,"dur":4041.0000000000005},{"text":"counter 10 but whenever in it counter","start":542519,"end":548279,"dur":5760},{"text":"six it stops the generation let's look","start":545120,"end":550720,"dur":5600},{"text":"at a real example use case and in this","start":548279,"end":552640,"dur":4361},{"text":"case we are going to be looking at","start":550720,"end":554959,"dur":4239},{"text":"summarization okay so here is an essay","start":552640,"end":558680,"dur":6040},{"text":"from Paul gram how to do great work I","start":554959,"end":561680,"dur":6721},{"text":"copied this essay to a Google Talk and","start":558680,"end":565640,"dur":6960},{"text":"it's about 27 pages so here I created a","start":561680,"end":568079,"dur":6399},{"text":"variable called text and copied all the","start":565640,"end":569920,"dur":4280},{"text":"text in here","start":568079,"end":572600,"dur":4521},{"text":"and we're going to ask Gro to summarize","start":569920,"end":575959,"dur":6039},{"text":"this since we're using the mixe model it","start":572600,"end":580279,"dur":7679},{"text":"has a context window of 32,000 tokens so","start":575959,"end":582480,"dur":6521},{"text":"this should be good enough the system","start":580279,"end":584200,"dur":3921},{"text":"prompt is you're a helpful assistant","start":582480,"end":586440,"dur":3960},{"text":"your job is to identify main themes in","start":584200,"end":589079,"dur":4879},{"text":"the given text and create summary","start":586440,"end":592600,"dur":6160},{"text":"provide the summary in 10 bullet points","start":589079,"end":595600,"dur":6521},{"text":"and the user contacts is the text that","start":592600,"end":598399,"dur":5799},{"text":"we copied and here I want to stream the","start":595600,"end":601880,"dur":6280},{"text":"response so let's run this now this","start":598399,"end":604360,"dur":5961},{"text":"is going to be real time as you can see","start":601880,"end":606680,"dur":4800},{"text":"this was pretty fast so let's run this","start":604360,"end":609360,"dur":5000},{"text":"again just to give you a sense of how","start":606680,"end":612959,"dur":6279},{"text":"fast it is so whenever this yellow arrow","start":609360,"end":615480,"dur":6120},{"text":"sign comes here that's when it actually","start":612959,"end":618640,"dur":5681},{"text":"starts generating the response so now","start":615480,"end":620800,"dur":5320},{"text":"it's sending it to the model and here's","start":618640,"end":623079,"dur":4439},{"text":"the response that you get so if you're","start":620800,"end":625000,"dur":4200},{"text":"run this multiple times you will get ","start":623079,"end":627600,"dur":4521},{"text":"different responses but the summary","start":625000,"end":630200,"dur":5200},{"text":"seems to convey ideas for from the essay","start":627600,"end":631760,"dur":4160},{"text":"another thing that I have noticed is","start":630200,"end":634560,"dur":4360},{"text":"this none at the end whenever we use the","start":631760,"end":637480,"dur":5720},{"text":"streaming API so let me show you another","start":634560,"end":639760,"dur":5200},{"text":"example like whenever I use the","start":637480,"end":642920,"dur":5440},{"text":"streaming API I always get this none at","start":639760,"end":645320,"dur":5560},{"text":"the end this might be something that the","start":642920,"end":648480,"dur":5560},{"text":"model is using as a stop token yeah you","start":645320,"end":651240,"dur":5920},{"text":"can repeatedly see that now when I use","start":648480,"end":654000,"dur":5520},{"text":"the same prompt with chap completion","start":651240,"end":656000,"dur":4760},{"text":"endpoint rather than the streaming I","start":654000,"end":658000,"dur":4000},{"text":"don't really see that n so if you're","start":656000,"end":660200,"dur":4200},{"text":"doing streaming just make sure that you","start":658000,"end":663399,"dur":5399},{"text":"are aware of that none character in this","start":660200,"end":665360,"dur":5160},{"text":"last example I want to show you how to","start":663399,"end":669079,"dur":5680},{"text":"use the gro API with streamlit the code","start":665360,"end":672120,"dur":6760},{"text":"that you see in here is an example code","start":669079,"end":674240,"dur":5161},{"text":"that the grock has provided in their","start":672120,"end":677560,"dur":5440},{"text":"GitHub repo and this is creating","start":674240,"end":679880,"dur":5640},{"text":"streamlit app that will enable you to","start":677560,"end":683240,"dur":5680},{"text":"chat with grock API the only difference","start":679880,"end":685399,"dur":5519},{"text":"is that in their requirements or text","start":683240,"end":689480,"dur":6240},{"text":"file they are using I think close to 50","start":685399,"end":691639,"dur":6240},{"text":"different packages you don't need all of","start":689480,"end":695040,"dur":5560},{"text":"them you just need these five packages","start":691639,"end":697000,"dur":5361},{"text":"so let me walk you through this code","start":695040,"end":699399,"dur":4359},{"text":"step by step first we importing all the","start":697000,"end":702079,"dur":5079},{"text":"required packages that include streamlit","start":699399,"end":705959,"dur":6560},{"text":"Gro as well as L chain so in this case","start":702079,"end":707519,"dur":5440},{"text":"we want the chatbot to remember the","start":705959,"end":709639,"dur":3680},{"text":"previous conversations and that is why","start":707519,"end":711519,"dur":4000},{"text":"we're using the conversation buffer","start":709639,"end":714480,"dur":4841},{"text":"window memory that will restrict how","start":711519,"end":717440,"dur":5921},{"text":"many previous conversations the bot can","start":714480,"end":719880,"dur":5400},{"text":"remember next we need to load our","start":717440,"end":724680,"dur":7240},{"text":"API key so we are using EnV file here","start":719880,"end":728200,"dur":8320},{"text":"that is storing our grock API key then","start":724680,"end":730399,"dur":5719},{"text":"we have our main function so first we","start":728200,"end":734360,"dur":6160},{"text":"load the grock API key this is just for","start":730399,"end":737920,"dur":7521},{"text":"some cosmetic purposes so the title","start":734360,"end":741320,"dur":6960},{"text":"is going to be chart with Gro and this","start":737920,"end":744680,"dur":6760},{"text":"is a line that you will see in the app","start":741320,"end":746000,"dur":4680},{"text":"next a couple of things for","start":744680,"end":748320,"dur":3640},{"text":"customization so you will be able to","start":746000,"end":750959,"dur":4959},{"text":"choose different models right now you","start":748320,"end":754639,"dur":6319},{"text":"can choose either the mixe model or Lama","start":750959,"end":758360,"dur":7401},{"text":"270 billion model we also want the user","start":754639,"end":760240,"dur":5601},{"text":"to have the control on how many previous","start":758360,"end":762560,"dur":4200},{"text":"conversations they want to they want the","start":760240,"end":765120,"dur":4880},{"text":"model to remember so you can be choose","start":762560,"end":768399,"dur":5839},{"text":"between 1 to 10 and after that we Define","start":765120,"end":771240,"dur":6120},{"text":"our conversation buffer vend memory","start":768399,"end":774079,"dur":5680},{"text":"object next there's going to be a user","start":771240,"end":777560,"dur":6320},{"text":"input button so user asks a question and","start":774079,"end":779360,"dur":5281},{"text":"based on our","start":777560,"end":781639,"dur":4078.9999999999995},{"text":"conversation memory length that we have","start":779360,"end":785079,"dur":5719},{"text":"chosen we are going to add that to","start":781639,"end":789040,"dur":7401},{"text":"history next we will create our chart","start":785079,"end":793399,"dur":8320},{"text":"object so we are passing on the API key","start":789040,"end":796639,"dur":7599},{"text":"the model name to this chart chat Croc","start":793399,"end":799160,"dur":5761},{"text":"this is basically internally using the","start":796639,"end":801279,"dur":4640},{"text":"same schema that I have shown you before","start":799160,"end":805120,"dur":5960},{"text":"in order to interact with the API and we","start":801279,"end":807480,"dur":6201},{"text":"create our conversation so we provide","start":805120,"end":810720,"dur":5600},{"text":"our llm all the memory that the model is","start":807480,"end":813199,"dur":5719},{"text":"supposed to have so that goes into this","start":810720,"end":815760,"dur":5040},{"text":"conversation chain and this is being","start":813199,"end":819279,"dur":6080},{"text":"created by using L chain next we're","start":815760,"end":821720,"dur":5960},{"text":"simply keeping track if there is a new","start":819279,"end":825800,"dur":6521},{"text":"user question in the text box then","start":821720,"end":828839,"dur":7119},{"text":"that will run this chain get a","start":825800,"end":832120,"dur":6320},{"text":"response show that response to the user","start":828839,"end":834720,"dur":5881},{"text":"and add that to history so that's pretty","start":832120,"end":836639,"dur":4519},{"text":"much it we're going to create a new","start":834720,"end":838399,"dur":3679},{"text":"virtual environment so we're going to","start":836639,"end":841600,"dur":4961},{"text":"use create DN then the name of the","start":838399,"end":843079,"dur":4680},{"text":"virtual environment in this case I'm","start":841600,"end":846399,"dur":4799},{"text":"going to call it YouTube I want to","start":843079,"end":848240,"dur":5161},{"text":"use the same virtual environment for all","start":846399,"end":851199,"dur":4800},{"text":"my YouTube videos then we going to","start":848240,"end":852759,"dur":4519},{"text":"define the python version that we want","start":851199,"end":855920,"dur":4721},{"text":"to use I already have this so I'm going","start":852759,"end":858920,"dur":6161},{"text":"to say no but I want to install all the","start":855920,"end":860600,"dur":4680},{"text":"required packages and for that we're","start":858920,"end":862959,"dur":4038.9999999999995},{"text":"going to use pip install dashr","start":860600,"end":866079,"dur":5479},{"text":"requirements. text now there are only","start":862959,"end":869240,"dur":6281},{"text":"five packages as I said so this pretty","start":866079,"end":871920,"dur":5841},{"text":"quick and after that we are going to run","start":869240,"end":873720,"dur":4480},{"text":"the app so in order to run the app we're","start":871920,"end":879040,"dur":7120},{"text":"going to use stram lit run groor","start":873720,"end":882920,"dur":9200},{"text":"chatore app and this will launch our app","start":879040,"end":886240,"dur":7200},{"text":"okay so here's the app on the left hand","start":882920,"end":888720,"dur":5800},{"text":"side we have this drop- down menu you","start":886240,"end":891720,"dur":5480},{"text":"can choose between llama and the Mixel","start":888720,"end":894519,"dur":5799},{"text":"model you can also change the","start":891720,"end":896759,"dur":5039},{"text":"conversational memory length so we're","start":894519,"end":898920,"dur":4401},{"text":"going to ask machine learning related","start":896759,"end":902959,"dur":6200},{"text":"question and the question is what ISM I","start":898920,"end":905160,"dur":6240},{"text":"have seen some issues with the streamlet","start":902959,"end":907959,"dur":5000},{"text":"app sometimes it's pretty slow in","start":905160,"end":909920,"dur":4760},{"text":"response so here's the response from the","start":907959,"end":913759,"dur":5800},{"text":"model it talks about swm it provides a","start":909920,"end":916120,"dur":6200},{"text":"simple explanation right so it seems to","start":913759,"end":918519,"dur":4760},{"text":"be working but what I have seen is this","start":916120,"end":921000,"dur":4880},{"text":"streamlet app that they have provided","start":918519,"end":923279,"dur":4760},{"text":"sometimes doesn't work at all there","start":921000,"end":924920,"dur":3920},{"text":"might be some issues with the ","start":923279,"end":927240,"dur":3961},{"text":"integration with length chain I will","start":924920,"end":930399,"dur":5479},{"text":"create a followup videos and explore it","start":927240,"end":934000,"dur":6760},{"text":"further so play around with the API it's","start":930399,"end":937079,"dur":6680},{"text":"free for the time being and as we saw in","start":934000,"end":939680,"dur":5680},{"text":"this video it's extremely fast if you","start":937079,"end":941880,"dur":4801},{"text":"need help in building applications on","start":939680,"end":946160,"dur":6480},{"text":"the grock API you can reach out details","start":941880,"end":948880,"dur":7000},{"text":"are in the video description I will be","start":946160,"end":952120,"dur":5960},{"text":"creating more videos on Gro API because","start":948880,"end":955199,"dur":6319},{"text":"this really enables real time","start":952120,"end":958880,"dur":6760},{"text":"conversation with these llms so if you","start":955199,"end":962319,"dur":7120},{"text":"working on an llm related project I do","start":958880,"end":964880,"dur":6000},{"text":"offer Consulting and Advising Services","start":962319,"end":968360,"dur":6041},{"text":"both to startups as well as individuals","start":964880,"end":970120,"dur":5240},{"text":"check out the video description if","start":968360,"end":972440,"dur":4080},{"text":"that's something you are interested in I","start":970120,"end":974720,"dur":4600},{"text":"hope you found this video useful thanks","start":972440,"end":977360,"dur":4920},{"text":"for watching and as always see you in","start":974720,"end":980600,"dur":5880},{"text":"the next one","start":977360,"end":980600,"dur":3240}],"vocabulary":"Groq API, LPUs, Mixtral-8x7B, Discord, PromptEngineering, Patreon, Consulting, engineerprompt, localGPT, Groq, Colab Notebook, Streamlit, LangChain, LLMs, Midjourney, AI Image Generation ","summary":"This video demonstrates the Groq API, a fast inference API for LLMs.  The presenter shows how to access the free API and build example applications like chatbots using Python.  They highlight the speed of the Groq API, particularly with the Mixtral-8x7B model, and demonstrate various features like system messages, parameters, and stop sequences.  Code examples for using the API in Colab Notebooks and Streamlit are provided, along with a discussion of streaming responses.  The video also discusses using LangChain and  Midjourney/AI Image Generation in conjunction with the Groq API.\n","punctuatedText":"You probably remember Groq, the company building Language Processing Units (LPUs) for fast inference of LLMs.  Well, they've just started rolling out API access to developers.  They claim nearly 500 tokens per second for the Mixtral-8x7B model, and in my testing, that seems to be true. In this video, I'll show you how to access the Groq API for free and then show a couple of example use cases, building a chat box ‚Äì and this thing is crazy fast.  More on that later in the video.\n\nTo get access to the API, you'll need to go to gro.com, log in with your email, or, as in my case, with your Google account.  Apart from the API, they've also opened up a playground where you can test two models currently: one is the Llama 2-70B model, and the other is the Mixtral model.  They are also providing really detailed documentation.  More on this in a little bit. You'll be able to create your API keys here.\n\nBefore working with the API, let's have a quick look at the playground. You can provide your system message here, and user input here.  You have two different options for the model. For our experiments, we're going to be using the Mixtral model.  My system message: \"You are a helpful assistant, answer as John Snow.\"  The user input: \"Explain the importance of low-latency LLMs, since we are testing the speed of the Groq API.\"  I think this is an appropriate question.  Then you can also set other parameters. This will control the model's behavior. You can set the temperature, maximum new tokens to generate, also top-p, and if you want to include any stop sequence. Hit submit, and this is the real-time speed you'll get from the API.\n\nLet's have a quick look at the response.  \"Greetings, as John Snow, I am not an expert in low-latency large language models, but I can certainly try to explain their importance in a simple manner.\"  And then it goes on to generate a response. In this video, we are not really interested in the accuracy of the responses; we will only be concerned about the speed. That's why I'm not going to go over the responses the model generates.\n\nSo, let's say you come up with parameters for the model's behavior.  After that, you can click on the \"View Code\" button. This will give you Python code on how to start calling this API. There's also code available to call the API within JavaScript or even JSON.\n\nNext, I'm going to walk you through a few examples. For that, we need to create a new API key. Click on \"Create API Key,\" provide the API key name (let's say \"Groq One\"), and if you hit submit, this will show you your API key.  This API key should be stored securely.  . I'm going to delete this one because I'm using an existing API key.  Let me first walk you through the basic structure of working with Groq. I'll show you using a Google Colab Notebook.\n\nBefore that, we need to install the package using pip.  So, we'll use `pip install Groq`. This will install the package for us.\n\nAfter that, to use the package, we need to import it. We'll import the `os` module, then the Groq package.\n\nNext, we need to create a client.  This is the Groq client we'll use.  You'll need to provide your API key.  Since I'm using a notebook, I'll set the environment variable within the notebook. I'll show you how to do that in a bit.\n\nOnce you create the client, the rest of the process is very similar to calling the OpenAI API.  We'll use the chat completion endpoint.\n\nCreate a new message. Define the role‚Äîthe role is \"user\"‚Äîfor directly interacting with the model. Later, I'll show you how to define a \"system\" role so you can provide a system message.\n\nNext, provide your prompt from the user.  Then, select the model you want to use. In this case, we're using the Mixtral-8x7B model.\n\nOnce you make the call, you can get the model's response using the `choices` variable, then `messages` and then the `content` of the message.  That's how the basic structure of API usage will look.\n\nHere's a Google Colab Notebook I'm using. This is the exact same code. Let's first set an environment variable.\n\nClick the \"Secrets\" button, then create or add a new secret. I've provided my Groq API key here.  Provide the name of the secret (or environment variable) and the corresponding value.\n\nMake sure to toggle the \"Notebook Access\" button so your notebook can access the key.\n\nThis is the exact same code we saw.\n\nNow, let me show you the speed of generation in real time. I'll click this button to start generating the response.\n\nYou'll see it generating, almost as if it's streaming, but the whole thing was generated at once.  This is incredibly fast; I haven't seen anything like it before. Later, I'll show you how to enable streaming.  However, for this example, you don't need it.\n\nNow, let's look at adding a system message.  In that case, simply create a new role called \"system.\" This will be our system message. We will then instruct the model to act as Jon Snow. The user message or user role remains the same.  .  There are quite a few other options you can set.  For example, you can set the temperature. This will control the creativity or randomness of your output.  What is the maximum number of tokens the model can generate? Or, you want the model to generate next is top P. This will basically control the sampling mechanism through which it's generating output.  If you want to use any specific stop words, you can define those here.  And whether you want to stream the responses or not.  So, if you enable streaming, you will need to change the way you get the output. I'm going to show you that later in the video. But we are going to use the same prompt: Explain the importance of low-latency LLMs. Let me show you the real-time response that we get.  So, here's the speed at which it was able to generate the output, which is pretty amazing.  And it's also actually sticking to the character that we asked it to, so that's pretty nice as well.\n\nNext, let's look at streaming responses.  Here, we're using the same structure again.  I'm using the previous client that I created, so not really creating another client.  The difference you will see here is this: we enabled streaming.  Now that means that the model is not going to generate the whole response all together, but it will create this in chunks.  So, what we need to do is take one of those chunks at a time and show those to the user.  That's why the mechanism of printing is different than what we were using before. For now, let's look at the speed at which it streams data. This is real-time streaming for you. As you can see, this opens up so many possibilities. For example, with this, you will be able to have speech communication with the LLM. So, you can have a speech-to-text model which converts your speech into text, feeds this through the Groq API, gets a response in near real-time, and then converts it back from text to speech using another model.\n\nNext, we will look at how to use stop sequences. In this case, we want the model to stop generation if it encounters a \"6\" in its output. The prompt is \"Count to 10. Your response must begin with 'one'\". We give it an example as well, like how the generation is supposed to look like. Stop sequences are really helpful, specifically if you want to interrupt the model's generation in the middle.  So, this is actually interrupting the generation, although it's supposed to count to 10, but whenever it encounters \"6\", it stops the generation. Let's look at a real example use case.  In this case, we're going to look at summarization. Here is an essay, \"How to Do Great Work,\" by Paul S. Graham.  .  I copied this essay into a Google Doc, and it's about 27 pages long.  Here, I've created a variable called `text` and copied all the text into it.  We're going to ask Groq to summarize this.  Since we're using the Mixtral-8x7B model, it has a context window of 32,000 tokens, so this should be sufficient.\n\nThe system prompt is: \"You are a helpful assistant. Your job is to identify the main themes in the given text and create a summary. Provide the summary in 10 bullet points.\"  The user context is the text we copied.  Here, I want to stream the response. Let's run this now.\n\nThis will be real-time, as you can see.  It was quite fast.  Let's run it again to show how fast it is.  Whenever the yellow arrow appears, that's when the response begins to generate.  Now, it's sending it to the model. Here's the response you get.\n\nIf you run this multiple times, you'll get different responses. However, the summaries seem to convey the essay's ideas.\n\nAnother observation:  a \"None\" value appears at the end whenever we use the streaming API.  Let me show you another example.  Whenever I use the streaming API, I always get this \"None\" at the end.  This might be a stop token used by the model.  You'll repeatedly see this.\n\nNow, when I use the same prompt with the completion endpoint rather than streaming, I don't see that \"None\". So, if you're using streaming, be aware of the \"None\" character at the end.\n\nIn this final example, I want to show you how to use the Groq API with Streamlit. The code you see here is an example from the Groq GitHub repository.  This is creating a Streamlit app that will let you chat with the Groq API. The only difference is that their requirements or text file uses almost 50 packages.  You don't need all of them; just these five packages:\n\n*   Let me walk you through this code step-by-step. First, we're importing the required packages, including Streamlit, Groq, and LangChain. In this case, we want the chatbot to remember previous conversations. That's why we're using the conversation buffer window memory. This will limit how many previous conversations the bot remembers.\n*   Next, we need to load our API key. We're using an `.env` file to store the Groq API key.\n*   We have our main function.  First, we load the Groq API key. This is just for cosmetic purposes.  The title will be \"Chat with Groq.\"  This is a line you will see in the app.  Next, a couple of customization options so you can choose different models.  . Right now, you can choose either the Mixtral-8x7B model or the Llama 2 70 billion model.  We also want the user to have control over how many previous conversations they want the model to remember.  You can choose between 1 and 10.\n\nAfter that, we define our conversation buffer and memory object.\n\nNext, there's going to be a user input button.  So, the user asks a question.  Based on our conversation memory length, which we have chosen, we are going to add that to the history.\n\nNext, we will create our chat object.  So, we are passing in the API key and the model name to this chat object.  This is basically internally using the same schema that I have shown you before in order to interact with the Groq API.  We create our conversation.  We provide our LLM with all the memory that the model is supposed to have.  This goes into this conversation chain, and this is being created by using LangChain.\n\nNext, we are simply keeping track if there is a new user question in the text box.  Then, that will run this chain, get a response, show that response to the user, and add that to the history.  That's pretty much it.\n\nWe're going to create a new virtual environment.  So, we're going to use `create` then the name of the virtual environment. In this case, I'm going to call it \"YouTube.\" I want to use the same virtual environment for all my YouTube videos.\n\nThen, we're going to define the Python version that we want to use.  I already have this, so I'm going to say no.  But I want to install all the required packages.  For that, we're going to use `pip install -r requirements.txt`.\n\nNow, there are only five packages, as I said, so this is pretty quick.\n\nAfter that, we are going to run the app.  In order to run the app, we're going to use Streamlit to run the Groq Chat app. This will launch our app.\n\nOkay, so here's the app. On the left-hand side, we have this drop-down menu.  You can choose between Llama and the Mixtral model.  You can also change the conversational memory length.\n\nSo, we're going to ask a machine learning-related question.  The question is: \"What is SWM?\"\n\nI have seen some issues with the Streamlit app sometimes. It's pretty slow in response.  Here's the response from the model. It talks about SWM and provides a simple explanation.  Right, so it seems to be working.\n\nBut what I have seen is this Streamlit app that they have provided sometimes doesn't work at all. There might be some issues with the integration with LangChain.\n\nI will create follow-up videos and explore it further.  So, play around with the Groq API. It's free for the time being, and as we saw in this video, it's extremely fast.  If you need help building something, let me know. Applications on the Groq API are available.  .  Details are in the video description.  I will be creating more videos on the Groq API, as this truly enables real-time conversation with these LLMs.  If you are working on an LLM-related project, I offer Consulting and Advising Services to both startups and individuals.  Check the video description if this is something you are interested in.\n\nI hope you found this video useful.  Thanks for watching, and as always, see you in the next one.\n"}}